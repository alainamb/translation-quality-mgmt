{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Translation Error Annotation Agreement Analysis\n",
        "\n",
        "This notebook analyzes inter-annotator agreement on translation error annotations following the [MQM (Multidimensional Quality Metrics)](https://themqm.org/error-types-2/typology/) typology.\n",
        "\n",
        "In this notebook, we'll calculate:\n",
        "- Visualizations of agreement on the text\n",
        "- Exact matching for span boundaries\n",
        "- F1 for partial credit on spans\n",
        "- Kappa for category agreement, subcategory agreement, and severity levels"
      ],
      "metadata": {
        "id": "osEzwam1IhG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. SETUP AND IMPORTS"
      ],
      "metadata": {
        "id": "waDDU19iIw-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7-MRS5QIWmF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib  # Add base matplotlib import\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import cohen_kappa_score, f1_score\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import re\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Print version information with error handling\n",
        "print(f\"pandas version: {pd.__version__}\")\n",
        "print(f\"numpy version: {np.__version__}\")\n",
        "try:\n",
        "    print(f\"matplotlib version: {matplotlib.__version__}\")\n",
        "except (ImportError, NameError, AttributeError):\n",
        "    print(\"matplotlib not properly imported\")\n",
        "try:\n",
        "    print(f\"seaborn version: {sns.__version__}\")\n",
        "except (ImportError, NameError, AttributeError):\n",
        "    print(\"seaborn not properly imported\")\n",
        "try:\n",
        "    import sklearn\n",
        "    print(f\"scikit-learn version: {sklearn.__version__}\")\n",
        "except (ImportError, NameError, AttributeError):\n",
        "    print(\"scikit-learn not properly imported\")\n",
        "\n",
        "# Install required packages if in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    print(\"Running in Google Colab. Installing/upgrading required packages...\")\n",
        "    !pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab\")\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('ggplot')\n",
        "sns.set_palette(\"colorblind\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. DATA LOADING"
      ],
      "metadata": {
        "id": "yt1Pr2CJI8kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we load the data from Google Drive. The setup of the using_colab variable handles data loading intelligently. The code uses a try/except block to detect whether the code is running in Google Colab.\n",
        "\n",
        "If the code is running in Colab:\n",
        "- It tries to import the Google Colab drive module\n",
        "- Mounts your Google Drive to /content/drive\n",
        "- Sets using_colab = True\n",
        "- Prints a success message\n",
        "\n",
        "If the code is not running in Colab:\n",
        "- The import will fail with an ImportError\n",
        "- The except block catches this error\n",
        "- Sets using_colab = False\n",
        "- Prints a message indicating you’re using the local file system\n",
        "\n",
        "This pattern makes the notebook work in both environments."
      ],
      "metadata": {
        "id": "aX6ITS1ieu7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive for Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    using_colab = True\n",
        "    print(\"Google Drive mounted successfully\")\n",
        "except ImportError:\n",
        "    using_colab = False\n",
        "    print(\"Not running in Google Colab, using local file system\")"
      ],
      "metadata": {
        "id": "mwM-cu98LNSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, the code sets up file paths for the translation annotation project and checks whether these paths exist in the file system. Here’s a breakdown of what it does:\n",
        "1. It imports the os module to work with file paths and directories.\n",
        "2. It defines a check_path function that:\n",
        "- Checks if a given path exists\n",
        "- Prints whether the path exists or not\n",
        "- Returns a boolean indicating existence\n",
        "\n",
        "3. It sets up paths to data files based on the environment:\n",
        "- If running in Google Colab (using_colab is True), it uses paths in Google Drive\n",
        "- Otherwise, it uses local paths\n",
        "\n",
        "4. It prints the paths to the annotations and content directories.\n",
        "5. It checks if these directories exist using the check_path function. If the annotation directory exists, it:\n",
        "- Lists all files in the directory\n",
        "- Creates a variable containing an array of all JSON files in the directory\n",
        "- Prints how many JSON files were found"
      ],
      "metadata": {
        "id": "cN2Xe4PPhW9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the check_path function\n",
        "def check_path(path):\n",
        "    exists = os.path.exists(path)\n",
        "    print(f\" - {path}: {'EXISTS' if exists else 'DOES NOT EXIST'}\")\n",
        "    return exists\n",
        "\n",
        "# Set paths to your data files\n",
        "if using_colab:\n",
        "    # Replace 'your_project_folder' with the actual folder name in your Google Drive\n",
        "    base_path = '/content/drive/Translation_Annotations'\n",
        "    annotation_dir = f'{base_path}/annotations'\n",
        "    content_dir = f'{base_path}/content'\n",
        "else:\n",
        "    # Local paths\n",
        "    annotation_dir = './data/annotations'\n",
        "    content_dir = './data/content'\n",
        "\n",
        "print(f\"Annotation directory: {annotation_dir}\")\n",
        "print(f\"Content directory: {content_dir}\")\n",
        "\n",
        "# Check if the directories exist\n",
        "print(\"Checking paths:\")\n",
        "annotation_dir_exists = check_path(annotation_dir)\n",
        "content_dir_exists = check_path(content_dir)\n",
        "\n",
        "# List files in the directories if they exist\n",
        "if annotation_dir_exists:\n",
        "    print(f\"\\nFiles in annotation directory:\")\n",
        "    for file in os.listdir(annotation_dir):\n",
        "        print(f\" - {file}\")\n",
        "    # Create a variable containing an array of all JSON files in the annotations directory\n",
        "    annotation_files = [os.path.join(annotation_dir, f) for f in os.listdir(annotation_dir) if f.endswith('.json')]\n",
        "    # The variable annotation_files contains an array of file paths to all JSON files in the annotation directory.\n",
        "    print(f\"\\nFound {len(annotation_files)} JSON files in the annotations directory\")\n",
        "\n",
        "if content_dir_exists:\n",
        "    print(f\"\\nFiles in content directory:\")\n",
        "    for file in os.listdir(content_dir):\n",
        "        print(f\" - {file}\")\n",
        "    # Create a variable containing an array of all JSON files in the content directory\n",
        "    content_files = [os.path.join(content_dir, f) for f in os.listdir(content_dir) if f.endswith('.json')]\n",
        "    # The variable content_files contains an array of file paths to all JSON files in content directory.\n",
        "    print(f\"\\nFound {len(content_files)} JSON files in the content directory\")"
      ],
      "metadata": {
        "id": "TAeHKmIReBlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. DATA PRE-PROCESSING"
      ],
      "metadata": {
        "id": "ZMhLsq5BhMZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we’re processing JSON files exported from Label Studio after texts have been annotated following the MQM translation error typology."
      ],
      "metadata": {
        "id": "aWEhON-oAh1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Document Identification\n",
        "The annotation objects in the JSON files give a unique ID for the text that was annotated for that object. That is, texts that are the same are given unique IDs, preventing the use of the document ID to group texts annotated by multiple annotators for analysis.\n",
        "\n",
        "The code below therefore:\n",
        "- Creates a way to identify unique texts across JSON files using section headers\n",
        "- Groups annotation objects by assigning consistent doc_ids to each unique text\n",
        "- Creates a verification code that shows which texts were identified and how many annotations exist for each"
      ],
      "metadata": {
        "id": "vomiv3dm_8_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "def identify_documents(annotation_files):\n",
        "    \"\"\"\n",
        "    Process annotation files to identify unique documents based on section headers.\n",
        "\n",
        "    Args:\n",
        "        annotation_files: List of paths to JSON annotation files\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - section_to_doc_id: Dictionary mapping section headers to doc_ids\n",
        "        - doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "    \"\"\"\n",
        "    # Dictionary to map section headers to unique doc_ids\n",
        "    section_to_doc_id = {}\n",
        "\n",
        "    # Dictionary to store annotations by doc_id\n",
        "    doc_id_to_annotations = {}\n",
        "\n",
        "    # Base regex pattern to extract section header (after the main title)\n",
        "    header_pattern = r\"BLUE CARBON, MANGROVES AND PUBLIC POLICY\\s*\\n+\\s*([A-Z][A-Z\\s:]+(?:\\s+AND\\s+[A-Z\\s]+)?)\"\n",
        "\n",
        "    # Process each annotation file\n",
        "    for file_path in annotation_files:\n",
        "        try:\n",
        "            print(f\"\\nReading file: {os.path.basename(file_path)}\")\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            print(f\"File contains {len(data)} annotation entries\")\n",
        "\n",
        "            # Process each annotation in the file\n",
        "            for item in data:\n",
        "                # Extract the document text\n",
        "                text = item.get('text', '')\n",
        "\n",
        "                # Extract section header using regex\n",
        "                match = re.search(header_pattern, text)\n",
        "                if match:\n",
        "                    section_header = match.group(1).strip()\n",
        "\n",
        "                    # Remove any trailing single letters that might be separated by whitespace\n",
        "                    section_header = re.sub(r'\\s+[A-Z]\\s*$', '', section_header)\n",
        "                    # Also remove trailing single letters without whitespace\n",
        "                    section_header = re.sub(r'[A-Z]$', '', section_header) if len(section_header) > 1 else section_header\n",
        "                    # Trim any resulting whitespace\n",
        "                    section_header = section_header.strip()\n",
        "\n",
        "                    # For \"A GREAT CARBON STORAGE SYSTEM\", add \": MANGROVES\" if it exists in the text\n",
        "                    if section_header == \"A GREAT CARBON STORAGE SYSTEM\" and \": MANGROVES\" in text:\n",
        "                        section_header = \"A GREAT CARBON STORAGE SYSTEM: MANGROVES\"\n",
        "                else:\n",
        "                    # Fallback to using the first 50 chars if no section header found\n",
        "                    section_header = text[:50].strip()\n",
        "\n",
        "                print(f\"Found section header: '{section_header}'\")\n",
        "\n",
        "                # Check if we've seen this section before\n",
        "                if section_header not in section_to_doc_id:\n",
        "                    # Assign new doc_id\n",
        "                    doc_id = len(section_to_doc_id) + 1\n",
        "                    section_to_doc_id[section_header] = doc_id\n",
        "                    doc_id_to_annotations[doc_id] = []\n",
        "                    print(f\"  → Assigned NEW doc_id: {doc_id}\")\n",
        "                else:\n",
        "                    # Use existing doc_id\n",
        "                    doc_id = section_to_doc_id[section_header]\n",
        "                    print(f\"  → Using EXISTING doc_id: {doc_id}\")\n",
        "\n",
        "                # Store annotation with doc_id\n",
        "                item['doc_id'] = doc_id\n",
        "                doc_id_to_annotations[doc_id].append(item)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    return section_to_doc_id, doc_id_to_annotations"
      ],
      "metadata": {
        "id": "2N0WsWL27OYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the document identification using the existing annotation_files variable\n",
        "section_to_doc_id, doc_id_to_annotations = identify_documents(annotation_files)\n",
        "\n",
        "# To verify the results more thoroughly, examine a sample from each document:\n",
        "print(\"\\nFirst few words of each unique document:\")\n",
        "for doc_id, annotations in doc_id_to_annotations.items():\n",
        "    if annotations:\n",
        "        text_sample = annotations[0].get('text', '')[:100].replace('\\n', ' ')\n",
        "        print(f\"  Doc ID {doc_id}: \\\"{text_sample}...\\\"\")"
      ],
      "metadata": {
        "id": "-TkRlHEv6w8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Restructuring the Annotations\n",
        "\n",
        "Each annotation object within the JSON exports has the following structure:\n",
        "- `text`: The full text content being annotated (appears to be sections from a scientific article)\n",
        "- `id`: A unique identifier for each entry (76, 77, 78)\n",
        "- `label`: An array of specific text annotations with:\n",
        " - `start` and `end` positions (character indices)\n",
        " - The highlighted `text` segment\n",
        " - `labels` categorizing the issue (like \"Accuracy\", \"Terminology\", \"Style\")\n",
        "- The `subcategory` (e.g., \"TERM: Wrong term\", \"STYLE: Unnatural style\"), `severity` (\"Minor\" or \"Major\"), and `comments` (explanations and feedback for improvement for the error identified in the label object) keys each have their own separate arrays that correspond positionally to the items in the label array.\n",
        "- `document_issues`: Area containing comments on issues spanning the entire document\n",
        "- `overall_correspondence` and `overall_readability`: Numerical ratings (1-5 scale) of the quality of the content as a whole\n",
        "- `correspondence_comments` and `readability_comments`: Feedback on the quality of the text as a whole\n",
        "- Metadata including `annotator ID`, `annotation_id`, `timestamps`, and `lead_time`"
      ],
      "metadata": {
        "id": "hOAuv3v2Eehb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Pre-processing error-specific data**\n",
        "The code below combines the contents from the `subcategory`, `severity`, and `comments` arrays with the corresponding `label` array. Combining corresponding content for each span (`label`, `subcategory`, `severity`, `comments`) into a single label object will make it much easier to:\n",
        "- Calculate exact match metrics - With all data in a single object per span, you can directly compare span boundaries (`start`/`end`) between annotators.\n",
        "- Calculate F1 for partial credit - The restructured format makes it straightforward to determine overlapping spans between annotators and compute precision/recall for partially matching annotations.\n",
        "- Calculate Kappa statistics - Having `labels`, `subcategory`, and `severity` in the same object makes it simple to extract these fields and compute inter-annotator agreement using Cohen's Kappa or Fleiss' Kappa for multiple annotators."
      ],
      "metadata": {
        "id": "BpWRau0XLwJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enrich_label_annotations(annotation):\n",
        "    \"\"\"\n",
        "    Combine data from subcategory, severity, and comments arrays into the label array.\n",
        "\n",
        "    Args:\n",
        "        annotation: A single annotation object from the JSON data\n",
        "\n",
        "    Returns:\n",
        "        The annotation object with enriched label items\n",
        "    \"\"\"\n",
        "    # Get the arrays\n",
        "    labels = annotation.get('label', [])\n",
        "    subcategories = annotation.get('subcategory', [])\n",
        "    severities = annotation.get('severity', [])\n",
        "    comments = annotation.get('comments', [])\n",
        "\n",
        "    # Create a new enriched label array\n",
        "    enriched_labels = []\n",
        "\n",
        "    # Process each label item\n",
        "    for i, label_item in enumerate(labels):\n",
        "        # Create a copy of the label item to enrich\n",
        "        enriched_item = label_item.copy()\n",
        "\n",
        "        # Add subcategory if available, otherwise \"None\"\n",
        "        enriched_item['subcategory'] = subcategories[i] if i < len(subcategories) else \"None\"\n",
        "\n",
        "        # Add severity if available, otherwise \"None\"\n",
        "        enriched_item['severity'] = severities[i] if i < len(severities) else \"None\"\n",
        "\n",
        "        # Add comments if available, otherwise \"None\"\n",
        "        enriched_item['comments'] = comments[i] if i < len(comments) else \"None\"\n",
        "\n",
        "        # Add to the enriched labels array\n",
        "        enriched_labels.append(enriched_item)\n",
        "\n",
        "    # Replace the original label array with the enriched one\n",
        "    annotation['enriched_labels'] = enriched_labels\n",
        "\n",
        "    return annotation\n",
        "\n",
        "def process_all_annotations(doc_id_to_annotations):\n",
        "    \"\"\"\n",
        "    Process all annotations to enrich the label data.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "\n",
        "    Returns:\n",
        "        The updated doc_id_to_annotations dictionary\n",
        "    \"\"\"\n",
        "    # Keep track of statistics for verification\n",
        "    total_annotations = 0\n",
        "    total_original_labels = 0\n",
        "    total_enriched_labels = 0\n",
        "    empty_annotations = []\n",
        "\n",
        "    # Process each document's annotations\n",
        "    for doc_id, annotations in doc_id_to_annotations.items():\n",
        "        for i, annotation in enumerate(annotations):\n",
        "            # Count original labels\n",
        "            original_labels = annotation.get('label', [])\n",
        "            total_original_labels += len(original_labels)\n",
        "\n",
        "            # Track annotations with no labels\n",
        "            if not original_labels:\n",
        "                annotation_id = annotation.get('id', f\"Unknown-{doc_id}-{i}\")\n",
        "                empty_annotations.append({\n",
        "                    'doc_id': doc_id,\n",
        "                    'annotation_id': annotation_id,\n",
        "                    'metadata': {\n",
        "                        'annotator': annotation.get('annotator', 'Unknown'),\n",
        "                        'annotation_id': annotation.get('annotation_id', 'Unknown')\n",
        "                    }\n",
        "                })\n",
        "\n",
        "            # Enrich the annotation\n",
        "            enriched_annotation = enrich_label_annotations(annotation)\n",
        "\n",
        "            # Count enriched labels\n",
        "            enriched_labels = enriched_annotation.get('enriched_labels', [])\n",
        "            total_enriched_labels += len(enriched_labels)\n",
        "\n",
        "            # Update the annotation in the list\n",
        "            annotations[i] = enriched_annotation\n",
        "            total_annotations += 1\n",
        "\n",
        "    # Print verification statistics\n",
        "    print(f\"\\nProcessed {total_annotations} total annotations\")\n",
        "    print(f\"Total original label items: {total_original_labels}\")\n",
        "    print(f\"Total enriched label items: {total_enriched_labels}\")\n",
        "\n",
        "    # Print information about annotations with no labels\n",
        "    if empty_annotations:\n",
        "        print(f\"\\nFound {len(empty_annotations)} annotations with no label items:\")\n",
        "        for i, empty_ann in enumerate(empty_annotations, 1):\n",
        "            print(f\"  {i}. Doc ID: {empty_ann['doc_id']}, Annotation ID: {empty_ann['annotation_id']}\")\n",
        "    else:\n",
        "        print(\"\\nAll annotations contained label items - processing complete!\")\n",
        "\n",
        "    return doc_id_to_annotations\n",
        "\n",
        "# Run the processing on your annotations\n",
        "doc_id_to_annotations = process_all_annotations(doc_id_to_annotations)\n",
        "\n",
        "# Function to print enriched label items in a specific order\n",
        "def print_ordered_label(label_item):\n",
        "    \"\"\"Print a label item with keys in a specific order\"\"\"\n",
        "    # Define the desired order\n",
        "    order = ['start', 'end', 'text', 'labels', 'subcategory', 'severity', 'comments']\n",
        "\n",
        "    # Print each field in order (if it exists)\n",
        "    print(\"{\")\n",
        "    for key in order:\n",
        "        if key in label_item:\n",
        "            value = label_item[key]\n",
        "            # Format string values with quotes\n",
        "            if isinstance(value, str):\n",
        "                formatted_value = f\"'{value}'\"\n",
        "            else:\n",
        "                formatted_value = value\n",
        "            print(f\"    '{key}': {formatted_value},\")\n",
        "    print(\"}\")\n",
        "\n",
        "# Show a sample of the enriched data\n",
        "print(\"\\nSample of enriched label data:\")\n",
        "for doc_id, annotations in doc_id_to_annotations.items():\n",
        "    if annotations:\n",
        "        sample_annotation = annotations[0]\n",
        "        enriched_labels = sample_annotation.get('enriched_labels', [])\n",
        "\n",
        "        if enriched_labels and len(enriched_labels) > 0:\n",
        "            print(f\"\\nDoc ID {doc_id} - First enriched label item:\")\n",
        "            print_ordered_label(enriched_labels[0])\n",
        "            print(f\"Total enriched label items for this annotation: {len(enriched_labels)}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "VPFMYEYvHfWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Pre-processing Document-Level Data**\n",
        "\n",
        "The code below standardizes and enriches document-level annotation data, including overall ratings, comments, and timing metrics. This preprocessing addresses several challenges:\n",
        "\n",
        "- **Standardizing Inconsistent Structures**: Document-level ratings (`overall_correspondence`, `overall_readability`) appear in varied formats across annotations. This preprocessing extracts numeric values into a consistent structure.\n",
        "\n",
        "- **Handling Missing Data**: Fields like `document_issues`, `correspondence_comments`, and `readability_comments` are irregularly present. The preprocessing explicitly marks missing fields with `None`, which provides valuable training information for evaluators.\n",
        "\n",
        "- **Calculating Time Metrics**: Two time measurements are extracted:\n",
        "  - `lead_time`: The active work time tracked by the annotation system\n",
        "  - `review_time`: The elapsed time between creation and completion (computed from timestamps)\n",
        "  \n",
        "- **Organizing by Document**: Annotations are grouped by document ID, enabling document-level analysis and comparisons.\n",
        "\n",
        "This preprocessing enables several important analyses:\n",
        "1. **Evaluator Performance Assessment**: Identifying annotators who consistently omit required fields or take substantially less/more time than peers\n",
        "2. **Inter-Rater Reliability**: Calculating agreement metrics for overall correspondence and readability ratings\n",
        "3. **Document Difficulty Analysis**: Correlating document characteristics with annotation time and rating consistency\n",
        "4. **Efficiency Metrics**: Comparing active work time versus total elapsed time to identify workflow bottlenecks\n",
        "\n",
        "The resulting standardized structure makes it straightforward to create visualizations showing rating distributions, completion rates, and time statistics across documents and annotators."
      ],
      "metadata": {
        "id": "yAmSw14qLiik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def preprocess_document_level_data(doc_id_to_annotations):\n",
        "    \"\"\"\n",
        "    Standardize document-level data across all annotations.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "\n",
        "    Returns:\n",
        "        List of standardized document-level data dictionaries\n",
        "    \"\"\"\n",
        "    processed_data = []\n",
        "\n",
        "    # Process all annotations across all documents\n",
        "    for doc_id, annotations in doc_id_to_annotations.items():\n",
        "        for annotation in annotations:\n",
        "            # Create standardized structure\n",
        "            doc_data = {\n",
        "                \"annotation_id\": annotation.get(\"annotation_id\"),\n",
        "                \"doc_id\": doc_id,  # Use the doc_id from the dictionary key\n",
        "                \"annotator\": annotation.get(\"annotator\"),\n",
        "                \"lead_time\": annotation.get(\"lead_time\"),\n",
        "                \"lead_time_minutes\": round(annotation.get(\"lead_time\", 0) / 60, 2),\n",
        "                \"created_at\": annotation.get(\"created_at\"),\n",
        "                \"updated_at\": annotation.get(\"updated_at\")\n",
        "            }\n",
        "\n",
        "            # Calculate review time (difference between created_at and updated_at)\n",
        "            if annotation.get(\"created_at\") and annotation.get(\"updated_at\"):\n",
        "                try:\n",
        "                    created = datetime.datetime.fromisoformat(annotation[\"created_at\"].replace('Z', '+00:00'))\n",
        "                    updated = datetime.datetime.fromisoformat(annotation[\"updated_at\"].replace('Z', '+00:00'))\n",
        "                    review_time_seconds = (updated - created).total_seconds()\n",
        "                    doc_data[\"review_time\"] = review_time_seconds\n",
        "                    doc_data[\"review_time_minutes\"] = round(review_time_seconds / 60, 2)\n",
        "                except (ValueError, TypeError):\n",
        "                    doc_data[\"review_time\"] = None\n",
        "                    doc_data[\"review_time_minutes\"] = None\n",
        "            else:\n",
        "                doc_data[\"review_time\"] = None\n",
        "                doc_data[\"review_time_minutes\"] = None\n",
        "\n",
        "            # Extract ratings from nested structures if present\n",
        "            if \"overall_correspondence\" in annotation:\n",
        "                if isinstance(annotation[\"overall_correspondence\"], list) and len(annotation[\"overall_correspondence\"]) > 0:\n",
        "                    doc_data[\"correspondence_rating\"] = annotation[\"overall_correspondence\"][0].get(\"rating\")\n",
        "                else:\n",
        "                    doc_data[\"correspondence_rating\"] = None\n",
        "            else:\n",
        "                doc_data[\"correspondence_rating\"] = None\n",
        "\n",
        "            if \"overall_readability\" in annotation:\n",
        "                if isinstance(annotation[\"overall_readability\"], list) and len(annotation[\"overall_readability\"]) > 0:\n",
        "                    doc_data[\"readability_rating\"] = annotation[\"overall_readability\"][0].get(\"rating\")\n",
        "                else:\n",
        "                    doc_data[\"readability_rating\"] = None\n",
        "            else:\n",
        "                doc_data[\"readability_rating\"] = None\n",
        "\n",
        "            # Handle text fields\n",
        "            doc_data[\"document_issues\"] = annotation.get(\"document_issues\", None)\n",
        "            doc_data[\"correspondence_comments\"] = annotation.get(\"correspondence_comments\", None)\n",
        "            doc_data[\"readability_comments\"] = annotation.get(\"readability_comments\", None)\n",
        "\n",
        "            # Track missing fields for training feedback\n",
        "            doc_data[\"missing_fields\"] = []\n",
        "            if doc_data[\"correspondence_rating\"] is None:\n",
        "                doc_data[\"missing_fields\"].append(\"correspondence_rating\")\n",
        "            if doc_data[\"readability_rating\"] is None:\n",
        "                doc_data[\"missing_fields\"].append(\"readability_rating\")\n",
        "            if doc_data[\"document_issues\"] is None:\n",
        "                doc_data[\"missing_fields\"].append(\"document_issues\")\n",
        "            if doc_data[\"correspondence_comments\"] is None:\n",
        "                doc_data[\"missing_fields\"].append(\"correspondence_comments\")\n",
        "            if doc_data[\"readability_comments\"] is None:\n",
        "                doc_data[\"missing_fields\"].append(\"readability_comments\")\n",
        "\n",
        "            processed_data.append(doc_data)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def summarize_document_level_data(processed_data):\n",
        "    \"\"\"\n",
        "    Generate summary statistics for document-level data.\n",
        "\n",
        "    Args:\n",
        "        processed_data: Output from preprocess_document_level_data\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of summary statistics\n",
        "    \"\"\"\n",
        "    total_annotations = len(processed_data)\n",
        "    total_with_missing_fields = sum(1 for item in processed_data if item[\"missing_fields\"])\n",
        "\n",
        "    # Get list of unique document IDs\n",
        "    doc_ids = set(item.get('doc_id') for item in processed_data if item.get('doc_id') is not None)\n",
        "    total_documents = len(doc_ids)\n",
        "\n",
        "    # Per-document statistics\n",
        "    per_document_stats = {}\n",
        "    for doc_id in doc_ids:\n",
        "        doc_annotations = [item for item in processed_data if item.get('doc_id') == doc_id]\n",
        "        corr_ratings = [item.get('correspondence_rating') for item in doc_annotations\n",
        "                        if item.get('correspondence_rating') is not None]\n",
        "        read_ratings = [item.get('readability_rating') for item in doc_annotations\n",
        "                       if item.get('readability_rating') is not None]\n",
        "        lead_times = [item.get('lead_time') for item in doc_annotations\n",
        "                     if item.get('lead_time') is not None]\n",
        "\n",
        "        per_document_stats[doc_id] = {\n",
        "            'count': len(doc_annotations),\n",
        "            'correspondence_ratings': corr_ratings,\n",
        "            'readability_ratings': read_ratings,\n",
        "            'lead_times': lead_times\n",
        "        }\n",
        "\n",
        "    missing_field_counts = {\n",
        "        \"correspondence_rating\": sum(1 for item in processed_data if \"correspondence_rating\" in item[\"missing_fields\"]),\n",
        "        \"readability_rating\": sum(1 for item in processed_data if \"readability_rating\" in item[\"missing_fields\"]),\n",
        "        \"document_issues\": sum(1 for item in processed_data if \"document_issues\" in item[\"missing_fields\"]),\n",
        "        \"correspondence_comments\": sum(1 for item in processed_data if \"correspondence_comments\" in item[\"missing_fields\"]),\n",
        "        \"readability_comments\": sum(1 for item in processed_data if \"readability_comments\" in item[\"missing_fields\"])\n",
        "    }\n",
        "\n",
        "    # Lead time statistics (non-None values only)\n",
        "    lead_times = [item[\"lead_time\"] for item in processed_data if item[\"lead_time\"] is not None]\n",
        "    lead_time_stats = {\n",
        "        \"count\": len(lead_times),\n",
        "        \"min\": min(lead_times) if lead_times else None,\n",
        "        \"max\": max(lead_times) if lead_times else None,\n",
        "        \"mean\": sum(lead_times) / len(lead_times) if lead_times else None\n",
        "    }\n",
        "\n",
        "    # Review time statistics (non-None values only)\n",
        "    review_times = [item[\"review_time\"] for item in processed_data if item[\"review_time\"] is not None]\n",
        "    review_time_stats = {\n",
        "        \"count\": len(review_times),\n",
        "        \"min\": min(review_times) if review_times else None,\n",
        "        \"max\": max(review_times) if review_times else None,\n",
        "        \"mean\": sum(review_times) / len(review_times) if review_times else None\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"total_annotations\": total_annotations,\n",
        "        \"total_documents\": total_documents,\n",
        "        \"total_with_missing_fields\": total_with_missing_fields,\n",
        "        \"percentage_incomplete\": round(100 * total_with_missing_fields / total_annotations, 2) if total_annotations > 0 else 0,\n",
        "        \"missing_field_counts\": missing_field_counts,\n",
        "        \"per_document_stats\": per_document_stats,\n",
        "        \"lead_time_stats\": lead_time_stats,\n",
        "        \"review_time_stats\": review_time_stats\n",
        "    }\n",
        "\n",
        "def generate_readable_report(processed_data, summary):\n",
        "    \"\"\"\n",
        "    Generate a human-readable report of the preprocessing results.\n",
        "\n",
        "    Args:\n",
        "        processed_data: Output from preprocess_document_level_data\n",
        "        summary: Output from summarize_document_level_data\n",
        "\n",
        "    Returns:\n",
        "        String containing a formatted report\n",
        "    \"\"\"\n",
        "    # Start building the report\n",
        "    report_lines = []\n",
        "\n",
        "    # Overall summary\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"DOCUMENT-LEVEL DATA PREPROCESSING REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"\")\n",
        "\n",
        "    report_lines.append(f\"Processed {summary['total_annotations']} total annotations across {summary['total_documents']} documents\")\n",
        "\n",
        "    # Completeness information\n",
        "    complete_count = summary['total_annotations'] - summary['total_with_missing_fields']\n",
        "    report_lines.append(f\"\\nCOMPLETENESS SUMMARY:\")\n",
        "    report_lines.append(f\"- Complete annotations: {complete_count} ({100-summary['percentage_incomplete']}%)\")\n",
        "    report_lines.append(f\"- Incomplete annotations: {summary['total_with_missing_fields']} ({summary['percentage_incomplete']}%)\")\n",
        "\n",
        "    # Details of missing fields\n",
        "    report_lines.append(\"\\nMISSING FIELDS BREAKDOWN:\")\n",
        "    for field, count in summary['missing_field_counts'].items():\n",
        "        if count > 0:\n",
        "            report_lines.append(f\"- {field}: missing in {count} annotations ({round(100*count/summary['total_annotations'], 1)}%)\")\n",
        "\n",
        "    # Document breakdown\n",
        "    report_lines.append(\"\\nDOCUMENT BREAKDOWN:\")\n",
        "    for doc_id, doc_stats in summary['per_document_stats'].items():\n",
        "        report_lines.append(f\"\\nDocument ID {doc_id}:\")\n",
        "        report_lines.append(f\"- Total annotations: {doc_stats['count']}\")\n",
        "\n",
        "        if doc_stats['correspondence_ratings']:\n",
        "            avg_corr = sum(doc_stats['correspondence_ratings']) / len(doc_stats['correspondence_ratings'])\n",
        "            report_lines.append(f\"- Average correspondence rating: {avg_corr:.1f}\")\n",
        "        else:\n",
        "            report_lines.append(f\"- Average correspondence rating: No data\")\n",
        "\n",
        "        if doc_stats['readability_ratings']:\n",
        "            avg_read = sum(doc_stats['readability_ratings']) / len(doc_stats['readability_ratings'])\n",
        "            report_lines.append(f\"- Average readability rating: {avg_read:.1f}\")\n",
        "        else:\n",
        "            report_lines.append(f\"- Average readability rating: No data\")\n",
        "\n",
        "        if doc_stats['lead_times']:\n",
        "            avg_lead = sum(doc_stats['lead_times']) / len(doc_stats['lead_times']) / 60\n",
        "            report_lines.append(f\"- Average lead time: {avg_lead:.1f} minutes\")\n",
        "\n",
        "    # Time statistics\n",
        "    report_lines.append(\"\\nOVERALL TIME STATISTICS:\")\n",
        "\n",
        "    # Lead time\n",
        "    if summary['lead_time_stats']['count'] > 0:\n",
        "        avg_lead_min = round(summary['lead_time_stats']['mean'] / 60, 2)\n",
        "        min_lead_min = round(summary['lead_time_stats']['min'] / 60, 2)\n",
        "        max_lead_min = round(summary['lead_time_stats']['max'] / 60, 2)\n",
        "        report_lines.append(f\"- Lead time (active work time):\")\n",
        "        report_lines.append(f\"  * Average: {avg_lead_min} minutes\")\n",
        "        report_lines.append(f\"  * Range: {min_lead_min} to {max_lead_min} minutes\")\n",
        "\n",
        "    # Review time\n",
        "    if summary['review_time_stats']['count'] > 0:\n",
        "        avg_review_min = round(summary['review_time_stats']['mean'] / 60, 2)\n",
        "        min_review_min = round(summary['review_time_stats']['min'] / 60, 2)\n",
        "        max_review_min = round(summary['review_time_stats']['max'] / 60, 2)\n",
        "        report_lines.append(f\"- Review time (total elapsed time):\")\n",
        "        report_lines.append(f\"  * Average: {avg_review_min} minutes\")\n",
        "        report_lines.append(f\"  * Range: {min_review_min} to {max_review_min} minutes\")\n",
        "\n",
        "    # Sample of processed data\n",
        "    report_lines.append(\"\\nSAMPLE OF PROCESSED DATA:\")\n",
        "\n",
        "    # Select a complete sample if possible\n",
        "    complete_samples = [item for item in processed_data if not item[\"missing_fields\"]]\n",
        "    sample = complete_samples[0] if complete_samples else processed_data[0]\n",
        "\n",
        "    report_lines.append(f\"Annotation ID: {sample['annotation_id']}\")\n",
        "    report_lines.append(f\"Document ID: {sample.get('doc_id', 'N/A')}\")\n",
        "    report_lines.append(f\"Annotator: {sample.get('annotator', 'N/A')}\")\n",
        "\n",
        "    if sample.get('correspondence_rating') is not None:\n",
        "        report_lines.append(f\"Correspondence Rating: {sample['correspondence_rating']}\")\n",
        "\n",
        "    if sample.get('readability_rating') is not None:\n",
        "        report_lines.append(f\"Readability Rating: {sample['readability_rating']}\")\n",
        "\n",
        "    report_lines.append(f\"Lead Time: {sample.get('lead_time_minutes', 'N/A')} minutes\")\n",
        "    report_lines.append(f\"Review Time: {sample.get('review_time_minutes', 'N/A')} minutes\")\n",
        "\n",
        "    # Finish the report\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"PREPROCESSING COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    return \"\\n\".join(report_lines)\n",
        "\n",
        "def process_and_report(doc_id_to_annotations):\n",
        "    \"\"\"\n",
        "    Process the annotations and generate a human-readable report.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing (processed_data, summary, report)\n",
        "    \"\"\"\n",
        "    processed_data = preprocess_document_level_data(doc_id_to_annotations)\n",
        "    summary = summarize_document_level_data(processed_data)\n",
        "    report = generate_readable_report(processed_data, summary)\n",
        "\n",
        "    # Print the report\n",
        "    print(report)\n",
        "\n",
        "    return processed_data, summary, report\n",
        "\n",
        "# Example usage:\n",
        "# processed_data, summary, report = process_and_report(doc_id_to_annotations)"
      ],
      "metadata": {
        "id": "PPGQV8d7L3li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run document-level preprocessing on the same data used for label enrichment\n",
        "processed_data, summary, report = process_and_report(doc_id_to_annotations)"
      ],
      "metadata": {
        "id": "UrQSJfrsOQHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "swPaRk3RQbiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Overlaps in Translation Error Annotations - Code"
      ],
      "metadata": {
        "id": "k2MauBi8eQGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code produces a visualization of the annotated text, where annotations are highlighted and text highlighted in darker colors reflects that more annotators agreed that the span constituted an error. Hovering over the spans produces a pop up that states the number of annotators who marked that span, plus their categorization (label, subcategory, severity) and comments."
      ],
      "metadata": {
        "id": "52N27K7cHPbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_span_overlap_visualization(doc_id, doc_id_to_annotations):\n",
        "    \"\"\"\n",
        "    Create a visualization showing text with highlights based on annotation span overlap.\n",
        "\n",
        "    Args:\n",
        "        doc_id: The document ID to visualize\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "\n",
        "    Returns:\n",
        "        HTML string with the visualization\n",
        "    \"\"\"\n",
        "    # Get all annotations for this document\n",
        "    annotations = doc_id_to_annotations.get(doc_id, [])\n",
        "\n",
        "    if not annotations:\n",
        "        return f\"<p>No annotations found for document ID {doc_id}</p>\"\n",
        "\n",
        "    # Get the text content from the first annotation (all should have the same text)\n",
        "    text = annotations[0].get('text', '')\n",
        "\n",
        "    if not text:\n",
        "        return f\"<p>No text content found for document ID {doc_id}</p>\"\n",
        "\n",
        "    # Collect all spans from all annotators\n",
        "    all_spans = []\n",
        "    annotator_ids = set()\n",
        "\n",
        "    for annotation in annotations:\n",
        "        annotator = annotation.get('annotator')\n",
        "        annotator_ids.add(annotator)\n",
        "\n",
        "        enriched_labels = annotation.get('enriched_labels', [])\n",
        "        for label in enriched_labels:\n",
        "            start = label.get('start', 0)\n",
        "            end = label.get('end', 0)\n",
        "\n",
        "            # Skip invalid spans\n",
        "            if start >= end or start < 0 or end > len(text):\n",
        "                continue\n",
        "\n",
        "            all_spans.append({\n",
        "                'annotator': annotator,\n",
        "                'start': start,\n",
        "                'end': end,\n",
        "                'text': label.get('text', ''),\n",
        "                'labels': label.get('labels', []),\n",
        "                'subcategory': label.get('subcategory', ''),\n",
        "                'severity': label.get('severity', ''),\n",
        "                'comments': label.get('comments', '')\n",
        "            })\n",
        "\n",
        "    # Create an array to track spans at each character position\n",
        "    # Instead of just counts, we'll store the full span information at each position\n",
        "    char_spans = [[] for _ in range(len(text))]\n",
        "\n",
        "    # Mark each span in the character spans array\n",
        "    for span in all_spans:\n",
        "        for i in range(span['start'], span['end']):\n",
        "            if i < len(char_spans):\n",
        "                char_spans[i].append(span)\n",
        "\n",
        "    # Create a count array for the gradient calculations\n",
        "    char_count = [len(spans) for spans in char_spans]\n",
        "\n",
        "    # Find maximum overlap for normalization\n",
        "    max_overlap = max(char_count) if char_count else 1\n",
        "\n",
        "    # Generate HTML with spans for highlighting\n",
        "    html_parts = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        if char_count[i] > 0:\n",
        "            # Start of a highlighted region\n",
        "            overlap = char_count[i]\n",
        "            start_i = i\n",
        "\n",
        "            # Find where this level of overlap ends\n",
        "            while i < len(text) and char_count[i] == overlap:\n",
        "                i += 1\n",
        "\n",
        "            # Calculate highlight intensity (0 to 1)\n",
        "            intensity = overlap / max_overlap\n",
        "\n",
        "            # Generate color (lighter blue palette for better readability)\n",
        "            r = int(220 - (100 * intensity))\n",
        "            g = int(220 - (100 * intensity))\n",
        "            b = int(255 - (35 * intensity))\n",
        "\n",
        "            bg_color = f\"rgb({r}, {g}, {b})\"\n",
        "\n",
        "            # Create detailed tooltip that shows all annotations for this span\n",
        "            tooltip_content = []\n",
        "\n",
        "            # Get all unique spans in this region\n",
        "            span_positions = range(start_i, i)\n",
        "            region_spans = set()\n",
        "            for pos in span_positions:\n",
        "                for span in char_spans[pos]:\n",
        "                    # Use a tuple of annotator+start+end as a unique identifier for the span\n",
        "                    span_id = (span['annotator'], span['start'], span['end'])\n",
        "                    region_spans.add(span_id)\n",
        "\n",
        "            # Get the full span objects from their IDs\n",
        "            unique_spans = []\n",
        "            for span_id in region_spans:\n",
        "                for pos in span_positions:\n",
        "                    for span in char_spans[pos]:\n",
        "                        if (span['annotator'], span['start'], span['end']) == span_id and span not in unique_spans:\n",
        "                            unique_spans.append(span)\n",
        "                            break\n",
        "\n",
        "            # Build tooltip content for each unique span\n",
        "            for span in unique_spans:\n",
        "                span_info = f\"Annotator: {span['annotator']}\"\n",
        "\n",
        "                # Add labels if available\n",
        "                if span['labels']:\n",
        "                    labels_str = ', '.join(span['labels'])\n",
        "                    span_info += f\" | Labels: {labels_str}\"\n",
        "\n",
        "                # Add subcategory if available\n",
        "                if span['subcategory']:\n",
        "                    span_info += f\" | Subcategory: {span['subcategory']}\"\n",
        "\n",
        "                # Add severity if available\n",
        "                if span['severity']:\n",
        "                    span_info += f\" | Severity: {span['severity']}\"\n",
        "\n",
        "                # Add comments if available - tried both comments and comment fields\n",
        "                if 'comments' in span and span['comments']:\n",
        "                    # Escape any quotes in the comments to prevent breaking the HTML\n",
        "                    comments = span['comments'].replace('\"', '&quot;').replace(\"'\", \"&#39;\")\n",
        "                    span_info += f\" | Comments: {comments}\"\n",
        "\n",
        "                tooltip_content.append(span_info)\n",
        "\n",
        "            # Join all tooltip content with line breaks\n",
        "            tooltip = f\"{overlap} annotator(s)&#10;&#10;\" + \"&#10;\".join(tooltip_content)\n",
        "\n",
        "            # Create a span with appropriate styling and tooltip\n",
        "            span_text = text[start_i:i].replace('\\n', '<br>')\n",
        "            html_parts.append(\n",
        "                f'<span style=\"background-color: {bg_color};\" '\n",
        "                f'title=\"{tooltip}\">{span_text}</span>'\n",
        "            )\n",
        "        else:\n",
        "            # Non-highlighted text\n",
        "            start_i = i\n",
        "\n",
        "            # Find where non-highlighted region ends\n",
        "            while i < len(text) and char_count[i] == 0:\n",
        "                i += 1\n",
        "\n",
        "            # Add the non-highlighted text\n",
        "            span_text = text[start_i:i].replace('\\n', '<br>')\n",
        "            html_parts.append(span_text)\n",
        "\n",
        "    # Balanced HTML structure with \"just right\" spacing\n",
        "    html = f\"\"\"\n",
        "    <div style=\"font-family: Arial, sans-serif; line-height: 1.3; margin: 0; padding: 0;\">\n",
        "        <h3 style=\"margin: 0 0 4px 0;\">Document ID: {doc_id}</h3>\n",
        "        <div style=\"margin: 0 0 6px 0; white-space: nowrap;\">\n",
        "            <span><strong>Number of unique annotators:</strong> {len(annotator_ids)}</span>\n",
        "            <span style=\"margin-left: 10px;\"><strong>Total annotations:</strong> {len(all_spans)}</span>\n",
        "            <span style=\"margin-left: 10px;\"><strong>Maximum overlap:</strong> {max_overlap} annotator{'' if max_overlap == 1 else 's'}</span>\n",
        "        </div>\n",
        "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 4px 0;\">\n",
        "            {''.join(html_parts)}\n",
        "        </div>\n",
        "        <div style=\"margin: 6px 0 0 0;\">\n",
        "            <p style=\"margin: 0 0 5px 0;\"><strong>Legend:</strong></p>\n",
        "            <div style=\"display: flex; flex-wrap: wrap; gap: 5px;\">\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a color legend with centered text\n",
        "    for i in range(1, max_overlap + 1):\n",
        "        intensity = i / max_overlap\n",
        "        r = int(220 - (100 * intensity))\n",
        "        g = int(220 - (100 * intensity))\n",
        "        b = int(255 - (35 * intensity))\n",
        "        bg_color = f\"rgb({r}, {g}, {b})\"\n",
        "\n",
        "        html += f\"\"\"\n",
        "            <div style=\"background-color: {bg_color}; display: flex; align-items: center; justify-content: center; min-width: 100px; height: 40px; text-align: center; margin-right: 5px;\">\n",
        "                {i} annotator{'' if i == 1 else 's'}\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html += \"\"\"\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "def visualize_all_documents(doc_id_to_annotations, selected_doc_id=None):\n",
        "    \"\"\"\n",
        "    Generate visualizations for all documents or a specific document in the dataset.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "        selected_doc_id: Optional specific document ID to visualize\n",
        "\n",
        "    Returns:\n",
        "        HTML string with visualizations\n",
        "    \"\"\"\n",
        "    all_html = \"<h2 style='margin-bottom: 5px;'>Annotation Overlap Visualization</h2>\"\n",
        "\n",
        "    if selected_doc_id is not None:\n",
        "        # Visualize only the selected document\n",
        "        if selected_doc_id in doc_id_to_annotations:\n",
        "            all_html += create_span_overlap_visualization(selected_doc_id, doc_id_to_annotations)\n",
        "        else:\n",
        "            all_html += f\"<p>Document ID {selected_doc_id} not found in the dataset.</p>\"\n",
        "    else:\n",
        "        # Visualize all documents\n",
        "        # Sort document IDs to ensure consistent order\n",
        "        doc_ids = sorted(doc_id_to_annotations.keys())\n",
        "\n",
        "        for i, doc_id in enumerate(doc_ids):\n",
        "            all_html += create_span_overlap_visualization(doc_id, doc_id_to_annotations)\n",
        "            if i < len(doc_ids) - 1:  # Don't add hr after the last document\n",
        "                all_html += \"<hr style='margin: 12px 0;'>\"\n",
        "\n",
        "    return all_html\n",
        "\n",
        "# Add the code to display the visualizations\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Function to display all documents\n",
        "def display_annotation_overlaps(doc_id=None):\n",
        "    \"\"\"\n",
        "    Display annotation overlap visualizations.\n",
        "\n",
        "    Args:\n",
        "        doc_id: Optional specific document ID to visualize\n",
        "    \"\"\"\n",
        "    html_output = visualize_all_documents(doc_id_to_annotations, doc_id)\n",
        "    display(HTML(html_output))"
      ],
      "metadata": {
        "id": "xuWv72KXiXbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Overlaps in Translation Error Annotations - Visualizations"
      ],
      "metadata": {
        "id": "ookrK__zrOxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the visualization on all documents\n",
        "display_annotation_overlaps()"
      ],
      "metadata": {
        "id": "hVF5AeGSSrPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OR display a specific document (e.g., document ID 1)\n",
        "display_annotation_overlaps(doc_id=1)"
      ],
      "metadata": {
        "id": "i-HvGAVPaMUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Types Identified - Code"
      ],
      "metadata": {
        "id": "Kw68K7Oc1w1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, visualizations on the types of errors (label, subcategory, severity) identified are presented."
      ],
      "metadata": {
        "id": "m2BsPvI4NVaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_error_types(doc_id_to_annotations, doc_id=None, include_subcategories=False):\n",
        "    \"\"\"\n",
        "    Visualize the distribution of error types across annotations.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "        doc_id: Optional specific document ID to analyze (None for all documents)\n",
        "        include_subcategories: Whether to show subcategories (default: False)\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    # Filter annotations by document ID if specified\n",
        "    if doc_id is not None:\n",
        "        annotations = doc_id_to_annotations.get(doc_id, [])\n",
        "        title_suffix = f\" (Document ID: {doc_id})\"\n",
        "    else:\n",
        "        # Combine all annotations\n",
        "        annotations = []\n",
        "        for doc_annotations in doc_id_to_annotations.values():\n",
        "            annotations.extend(doc_annotations)\n",
        "        title_suffix = \" (All Documents)\"\n",
        "\n",
        "    # Extract error labels from all annotations\n",
        "    labels_data = []\n",
        "    subcategory_data = []\n",
        "\n",
        "    for annotation in annotations:\n",
        "        enriched_labels = annotation.get('enriched_labels', [])\n",
        "\n",
        "        for label_item in enriched_labels:\n",
        "            # Extract main error categories\n",
        "            main_labels = label_item.get('labels', [])\n",
        "\n",
        "            for main_label in main_labels:\n",
        "                labels_data.append({\n",
        "                    'doc_id': annotation.get('doc_id'),\n",
        "                    'annotator': annotation.get('annotator'),\n",
        "                    'error_type': main_label,\n",
        "                    'severity': label_item.get('severity', 'Unknown')\n",
        "                })\n",
        "\n",
        "            # Extract subcategory if available\n",
        "            subcategory = label_item.get('subcategory', 'None')\n",
        "            if subcategory and subcategory != 'None':\n",
        "                # Split subcategory (format is typically \"CATEGORY: Description\")\n",
        "                subcategory_parts = subcategory.split(':', 1)\n",
        "                if len(subcategory_parts) > 1:\n",
        "                    category_code = subcategory_parts[0].strip()\n",
        "                    description = subcategory_parts[1].strip()\n",
        "                else:\n",
        "                    category_code = subcategory\n",
        "                    description = subcategory\n",
        "\n",
        "                subcategory_data.append({\n",
        "                    'doc_id': annotation.get('doc_id'),\n",
        "                    'annotator': annotation.get('annotator'),\n",
        "                    'category': category_code,\n",
        "                    'description': description,\n",
        "                    'full_subcategory': subcategory,\n",
        "                    'severity': label_item.get('severity', 'Unknown')\n",
        "                })\n",
        "\n",
        "    # Create DataFrames\n",
        "    labels_df = pd.DataFrame(labels_data)\n",
        "    subcategory_df = pd.DataFrame(subcategory_data)\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"Error Type Analysis{title_suffix}\")\n",
        "    print(f\"Total annotations analyzed: {len(annotations)}\")\n",
        "    print(f\"Total error labels found: {len(labels_df)}\")\n",
        "    print(f\"Total error subcategories found: {len(subcategory_df)}\")\n",
        "\n",
        "    # If no error labels were found, return early\n",
        "    if len(labels_df) == 0:\n",
        "        print(\"No error labels found in the selected document(s).\")\n",
        "        return None\n",
        "\n",
        "    # Count the frequency of each main error type\n",
        "    main_counts = labels_df['error_type'].value_counts().reset_index()\n",
        "    main_counts.columns = ['Error Type', 'Count']\n",
        "\n",
        "    # Sort by count (descending)\n",
        "    main_counts = main_counts.sort_values('Count', ascending=False)\n",
        "\n",
        "    # Print main error type distribution\n",
        "    print(\"\\nMain Error Types Distribution:\")\n",
        "    for i, row in main_counts.iterrows():\n",
        "        print(f\"  {row['Error Type']}: {row['Count']} occurrences\")\n",
        "\n",
        "    # Create bar chart for main error types\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Use a colorblind-friendly palette\n",
        "    colors = sns.color_palette(\"colorblind\", len(main_counts))\n",
        "\n",
        "    # Create the bar chart\n",
        "    bars = plt.bar(main_counts['Error Type'], main_counts['Count'], color=colors)\n",
        "\n",
        "    # Add count labels on top of each bar\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                f'{int(height)}', ha='center', va='bottom')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Error Type', fontsize=12)\n",
        "    plt.ylabel('Count', fontsize=12)\n",
        "    plt.title(f'Distribution of Error Types{title_suffix}', fontsize=14)\n",
        "\n",
        "    # Rotate x-axis labels if there are many categories\n",
        "    if len(main_counts) > 5:\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add grid for better readability\n",
        "    plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Error type distribution by document if no specific doc_id was provided\n",
        "    if doc_id is None and 'doc_id' in labels_df.columns:\n",
        "        plt.figure(figsize=(14, 8))\n",
        "\n",
        "        # Count error types by document\n",
        "        doc_error_counts = labels_df.groupby(['doc_id', 'error_type']).size().reset_index(name='count')\n",
        "\n",
        "        # Create grouped bar chart\n",
        "        sns.barplot(x='doc_id', y='count', hue='error_type', data=doc_error_counts)\n",
        "\n",
        "        plt.title('Error Types by Document', fontsize=14)\n",
        "        plt.xlabel('Document ID', fontsize=12)\n",
        "        plt.ylabel('Count', fontsize=12)\n",
        "        plt.legend(title='Error Type')\n",
        "        plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # If subcategories are requested, visualize them too\n",
        "    if include_subcategories and len(subcategory_df) > 0:\n",
        "        # Analyze subcategories\n",
        "        # Group by category code and get counts\n",
        "        subcategory_counts = subcategory_df['category'].value_counts().reset_index()\n",
        "        subcategory_counts.columns = ['Category', 'Count']\n",
        "\n",
        "        # Get the top 10 subcategories (if there are more than 10)\n",
        "        if len(subcategory_counts) > 10:\n",
        "            print(\"\\nTop 10 Subcategories:\")\n",
        "            top_subcategories = subcategory_counts.head(10)\n",
        "        else:\n",
        "            print(\"\\nAll Subcategories:\")\n",
        "            top_subcategories = subcategory_counts\n",
        "\n",
        "        # Print subcategory distribution\n",
        "        for i, row in top_subcategories.iterrows():\n",
        "            print(f\"  {row['Category']}: {row['Count']} occurrences\")\n",
        "\n",
        "        # Create detailed subcategory breakdown\n",
        "        full_subcategory_counts = subcategory_df['full_subcategory'].value_counts().reset_index()\n",
        "        full_subcategory_counts.columns = ['Subcategory', 'Count']\n",
        "\n",
        "        # Get the top 15 full subcategories\n",
        "        if len(full_subcategory_counts) > 15:\n",
        "            top_full_subcategories = full_subcategory_counts.head(15)\n",
        "        else:\n",
        "            top_full_subcategories = full_subcategory_counts\n",
        "\n",
        "        # Create horizontal bar chart for better readability with long labels\n",
        "        plt.figure(figsize=(12, max(6, len(top_full_subcategories) * 0.4)))\n",
        "\n",
        "        # Create horizontal bar chart\n",
        "        bars = plt.barh(top_full_subcategories['Subcategory'],\n",
        "                      top_full_subcategories['Count'],\n",
        "                      color=sns.color_palette(\"colorblind\", len(top_full_subcategories)))\n",
        "\n",
        "        # Add count labels inside each bar\n",
        "        for i, bar in enumerate(bars):\n",
        "            width = bar.get_width()\n",
        "            plt.text(width - 0.5, bar.get_y() + bar.get_height()/2.,\n",
        "                    f'{int(width)}', ha='right', va='center',\n",
        "                    color='white', fontweight='bold')\n",
        "\n",
        "        plt.xlabel('Count', fontsize=12)\n",
        "        plt.ylabel('Subcategory', fontsize=12)\n",
        "        plt.title(f'Top Error Subcategories{title_suffix}', fontsize=14)\n",
        "        plt.grid(axis='x', linestyle=':', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Add breakdown by severity if there's enough data\n",
        "        if 'severity' in subcategory_df.columns and subcategory_df['severity'].nunique() > 1:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "\n",
        "            # Count by severity and category\n",
        "            severity_counts = subcategory_df.groupby(['category', 'severity']).size().reset_index(name='count')\n",
        "\n",
        "            # Get top categories\n",
        "            top_categories = subcategory_df['category'].value_counts().head(8).index\n",
        "            severity_counts = severity_counts[severity_counts['category'].isin(top_categories)]\n",
        "\n",
        "            # Create grouped bar chart\n",
        "            sns.barplot(x='category', y='count', hue='severity', data=severity_counts)\n",
        "\n",
        "            plt.title(f'Error Types by Severity{title_suffix}', fontsize=14)\n",
        "            plt.xlabel('Error Category', fontsize=12)\n",
        "            plt.ylabel('Count', fontsize=12)\n",
        "            plt.legend(title='Severity')\n",
        "            plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
        "\n",
        "            # Rotate x-axis labels if needed\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    return labels_df, subcategory_df if include_subcategories else None"
      ],
      "metadata": {
        "id": "8IZslffX18bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Types Identified - Visualizations"
      ],
      "metadata": {
        "id": "lmzVa5BI11cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic error type analysis for all documents\n",
        "visualize_error_types(doc_id_to_annotations);"
      ],
      "metadata": {
        "id": "dIl1tWFD2HoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis for a specific document\n",
        "visualize_error_types(doc_id_to_annotations, doc_id=1);"
      ],
      "metadata": {
        "id": "VwnSDKhh2R0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Include detailed subcategory analysis\n",
        "visualize_error_types(doc_id_to_annotations, include_subcategories=True);"
      ],
      "metadata": {
        "id": "MCY5aOCi2aYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specific document with subcategories\n",
        "visualize_error_types(doc_id_to_annotations, doc_id=2, include_subcategories=True);"
      ],
      "metadata": {
        "id": "Bo1NgR0L2j_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Annotator Data - Code"
      ],
      "metadata": {
        "id": "oBJzIqbnvYJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, summaries of annotator data that provide nuance to the analysis are presented, such as lead time and review time."
      ],
      "metadata": {
        "id": "hZaWTGEsNhYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_annotation_timing(processed_data, doc_id=None):\n",
        "    \"\"\"\n",
        "    Create a clear tabular summary of annotation timing statistics.\n",
        "\n",
        "    Args:\n",
        "        processed_data: Output from preprocess_document_level_data\n",
        "        doc_id: Optional specific document ID to analyze (None for all documents)\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Create a DataFrame from the processed data\n",
        "    df = pd.DataFrame(processed_data)\n",
        "\n",
        "    # Filter by document ID if specified\n",
        "    if doc_id is not None:\n",
        "        df = df[df['doc_id'] == doc_id]\n",
        "        title_suffix = f\" (Document ID: {doc_id})\"\n",
        "    else:\n",
        "        title_suffix = \" (All Documents)\"\n",
        "\n",
        "    print(f\"\\n==== ANNOTATION TIMING ANALYSIS{title_suffix} ====\\n\")\n",
        "\n",
        "    # Overall statistics\n",
        "    print(\"OVERALL TIMING STATISTICS:\")\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "    for time_type, column in [('Lead Time', 'lead_time_minutes'),\n",
        "                             ('Review Time', 'review_time_minutes')]:\n",
        "        data = df[column].dropna()\n",
        "\n",
        "        if len(data) > 0:\n",
        "            print(f\"\\n{time_type} (minutes):\")\n",
        "            print(f\"  Number of measurements: {len(data)}\")\n",
        "            print(f\"  Median: {data.median():.1f}\")\n",
        "            print(f\"  Mean: {data.mean():.1f}\")\n",
        "            print(f\"  Q1 (25%): {data.quantile(0.25):.1f}\")\n",
        "            print(f\"  Q3 (75%): {data.quantile(0.75):.1f}\")\n",
        "            print(f\"  Min: {data.min():.1f}\")\n",
        "            print(f\"  Max: {data.max():.1f}\")\n",
        "\n",
        "            # Calculate outliers by 1.5 IQR rule\n",
        "            q1, q3 = data.quantile(0.25), data.quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "            outliers = data[data > upper_bound]\n",
        "            if len(outliers) > 0:\n",
        "                print(f\"\\n  Outliers (> {upper_bound:.1f} min): {len(outliers)} values\")\n",
        "                print(f\"  Outlier values: {', '.join([f'{x:.1f}' for x in sorted(outliers)])}\")\n",
        "\n",
        "    # Per-document analysis if not filtered\n",
        "    if doc_id is None and 'doc_id' in df.columns:\n",
        "        print(\"\\n\\nTIMING STATISTICS BY DOCUMENT:\")\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "        doc_ids = sorted(df['doc_id'].unique())\n",
        "\n",
        "        # Create summary table data\n",
        "        table_data = []\n",
        "        for current_doc_id in doc_ids:\n",
        "            doc_df = df[df['doc_id'] == current_doc_id]\n",
        "\n",
        "            # Lead time stats\n",
        "            lead_times = doc_df['lead_time_minutes'].dropna()\n",
        "            lead_count = len(lead_times)\n",
        "            lead_median = lead_times.median() if lead_count > 0 else float('nan')\n",
        "            lead_q1 = lead_times.quantile(0.25) if lead_count > 0 else float('nan')\n",
        "            lead_q3 = lead_times.quantile(0.75) if lead_count > 0 else float('nan')\n",
        "            lead_min = lead_times.min() if lead_count > 0 else float('nan')\n",
        "            lead_max = lead_times.max() if lead_count > 0 else float('nan')\n",
        "\n",
        "            # Review time stats\n",
        "            review_times = doc_df['review_time_minutes'].dropna()\n",
        "            review_count = len(review_times)\n",
        "            review_median = review_times.median() if review_count > 0 else float('nan')\n",
        "            review_q1 = review_times.quantile(0.25) if review_count > 0 else float('nan')\n",
        "            review_q3 = review_times.quantile(0.75) if review_count > 0 else float('nan')\n",
        "            review_min = review_times.min() if review_count > 0 else float('nan')\n",
        "            review_max = review_times.max() if review_count > 0 else float('nan')\n",
        "\n",
        "            table_data.append({\n",
        "                'Doc ID': current_doc_id,\n",
        "                'Lead Count': lead_count,\n",
        "                'Lead Median': lead_median,\n",
        "                'Lead Q1-Q3': f\"{lead_q1:.1f}-{lead_q3:.1f}\" if lead_count > 0 else \"N/A\",\n",
        "                'Lead Min-Max': f\"{lead_min:.1f}-{lead_max:.1f}\" if lead_count > 0 else \"N/A\",\n",
        "                'Review Count': review_count,\n",
        "                'Review Median': review_median,\n",
        "                'Review Q1-Q3': f\"{review_q1:.1f}-{review_q3:.1f}\" if review_count > 0 else \"N/A\",\n",
        "                'Review Min-Max': f\"{review_min:.1f}-{review_max:.1f}\" if review_count > 0 else \"N/A\"\n",
        "            })\n",
        "\n",
        "        # Create and format DataFrame\n",
        "        summary_df = pd.DataFrame(table_data)\n",
        "\n",
        "        # Format numeric columns\n",
        "        for col in ['Lead Median', 'Review Median']:\n",
        "            summary_df[col] = summary_df[col].apply(lambda x: f\"{x:.1f}\" if not pd.isna(x) else \"N/A\")\n",
        "\n",
        "        # Print the table\n",
        "        print(\"\\nSummary by Document:\")\n",
        "        print(summary_df.to_string(index=False))\n",
        "\n",
        "        # Print outliers by document\n",
        "        print(\"\\nOutliers by Document:\")\n",
        "        for current_doc_id in doc_ids:\n",
        "            doc_df = df[df['doc_id'] == current_doc_id]\n",
        "\n",
        "            print(f\"\\nDocument {current_doc_id}:\")\n",
        "\n",
        "            # Check for lead time outliers\n",
        "            lead_times = doc_df['lead_time_minutes'].dropna()\n",
        "            if len(lead_times) >= 4:  # Need reasonable amount of data for outlier detection\n",
        "                q1, q3 = lead_times.quantile(0.25), lead_times.quantile(0.75)\n",
        "                iqr = q3 - q1\n",
        "                upper_bound = q3 + 1.5 * iqr\n",
        "                outliers = lead_times[lead_times > upper_bound]\n",
        "\n",
        "                if len(outliers) > 0:\n",
        "                    print(f\"  Lead Time outliers (> {upper_bound:.1f} min): {len(outliers)} values\")\n",
        "                    print(f\"  Values: {', '.join([f'{x:.1f}' for x in sorted(outliers)])}\")\n",
        "                else:\n",
        "                    print(\"  Lead Time: No outliers detected\")\n",
        "            else:\n",
        "                print(\"  Lead Time: Insufficient data for outlier detection\")\n",
        "\n",
        "            # Check for review time outliers\n",
        "            review_times = doc_df['review_time_minutes'].dropna()\n",
        "            if len(review_times) >= 4:\n",
        "                q1, q3 = review_times.quantile(0.25), review_times.quantile(0.75)\n",
        "                iqr = q3 - q1\n",
        "                upper_bound = q3 + 1.5 * iqr\n",
        "                outliers = review_times[review_times > upper_bound]\n",
        "\n",
        "                if len(outliers) > 0:\n",
        "                    print(f\"  Review Time outliers (> {upper_bound:.1f} min): {len(outliers)} values\")\n",
        "                    print(f\"  Values: {', '.join([f'{x:.1f}' for x in sorted(outliers)])}\")\n",
        "                else:\n",
        "                    print(\"  Review Time: No outliers detected\")\n",
        "            else:\n",
        "                print(\"  Review Time: Insufficient data for outlier detection\")\n",
        "\n",
        "    # Return timing data if needed for further analysis\n",
        "    timing_data = {\n",
        "        'lead_time': df['lead_time_minutes'].dropna().values,\n",
        "        'review_time': df['review_time_minutes'].dropna().values\n",
        "    }\n",
        "\n",
        "    return timing_data"
      ],
      "metadata": {
        "id": "ACHuY3v-v_Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Annotator Data - Summaries\n",
        "\n",
        "- Lead time: Active time to complete the task\n",
        "- Review time: Time from start time to end time"
      ],
      "metadata": {
        "id": "etvnKt3KvdHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show annotator distribution for a specific document\n",
        "analyze_annotator_distribution(processed_data, doc_id=1)"
      ],
      "metadata": {
        "id": "h-2soM4NwKSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show annotator distribution for a specific document\n",
        "analyze_annotator_distribution(processed_data, doc_id=2)"
      ],
      "metadata": {
        "id": "tjWKsCQT0_Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show annotator distribution for a specific document\n",
        "analyze_annotator_distribution(processed_data, doc_id=3)"
      ],
      "metadata": {
        "id": "9lzL-ROC1ByM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall summary for all documents\n",
        "summarize_annotation_timing(processed_data);"
      ],
      "metadata": {
        "id": "sLfhuRnxy7KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary for a specific document\n",
        "summarize_annotation_timing(processed_data, doc_id=2);"
      ],
      "metadata": {
        "id": "o-_6ugywxhBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall correspondence and readability scores - Code"
      ],
      "metadata": {
        "id": "BHbu4etUeacq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, visualizations on the overall correspondence and readability ratings given to a translation are generated."
      ],
      "metadata": {
        "id": "BOcx4MbbOBcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_rating_distributions(processed_data, doc_id=None, plot_type='all'):\n",
        "    \"\"\"\n",
        "    Visualize the distribution of correspondence and readability ratings.\n",
        "\n",
        "    Args:\n",
        "        processed_data: Output from preprocess_document_level_data\n",
        "        doc_id: Optional specific document ID to visualize (None for all documents)\n",
        "        plot_type: Type of plot to generate ('count', 'box', 'grouped', 'all')\n",
        "\n",
        "    Returns:\n",
        "        matplotlib figure(s)\n",
        "    \"\"\"\n",
        "    from matplotlib.gridspec import GridSpec\n",
        "\n",
        "    # Create a copy of the data to avoid modifying the original\n",
        "    data = processed_data.copy()\n",
        "\n",
        "    # Filter by document ID if specified\n",
        "    if doc_id is not None:\n",
        "        data = [item for item in data if item.get('doc_id') == doc_id]\n",
        "        title_suffix = f\" (Document ID: {doc_id})\"\n",
        "    else:\n",
        "        title_suffix = \" (All Documents)\"\n",
        "\n",
        "    # Extract correspondence and readability ratings\n",
        "    correspondence_ratings = [item.get('correspondence_rating') for item in data\n",
        "                             if item.get('correspondence_rating') is not None]\n",
        "    readability_ratings = [item.get('readability_rating') for item in data\n",
        "                          if item.get('readability_rating') is not None]\n",
        "\n",
        "    # Count the occurrences of each rating\n",
        "    corr_counts = pd.Series(correspondence_ratings).value_counts().sort_index()\n",
        "    read_counts = pd.Series(readability_ratings).value_counts().sort_index()\n",
        "\n",
        "    # Prepare a DataFrame for seaborn\n",
        "    ratings_df = pd.DataFrame({\n",
        "        'Rating Value': list(range(1, 5)) * 2,\n",
        "        'Count': [corr_counts.get(i, 0) for i in range(1, 5)] + [read_counts.get(i, 0) for i in range(1, 5)],\n",
        "        'Category': ['Correspondence'] * 4 + ['Readability'] * 4\n",
        "    })\n",
        "\n",
        "    # Long-format for box plots\n",
        "    long_df = pd.DataFrame({\n",
        "        'Rating Value': correspondence_ratings + readability_ratings,\n",
        "        'Category': ['Correspondence'] * len(correspondence_ratings) + ['Readability'] * len(readability_ratings)\n",
        "    })\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"Correspondence Ratings{title_suffix}:\")\n",
        "    print(f\"  Number of ratings: {len(correspondence_ratings)}\")\n",
        "    print(f\"  Mean: {np.mean(correspondence_ratings):.2f}\")\n",
        "    print(f\"  Median: {np.median(correspondence_ratings):.1f}\")\n",
        "    print(f\"  Mode: {pd.Series(correspondence_ratings).mode().values}\")\n",
        "    print(f\"  Distribution: {corr_counts.to_dict()}\")\n",
        "\n",
        "    print(f\"\\nReadability Ratings{title_suffix}:\")\n",
        "    print(f\"  Number of ratings: {len(readability_ratings)}\")\n",
        "    print(f\"  Mean: {np.mean(readability_ratings):.2f}\")\n",
        "    print(f\"  Median: {np.median(readability_ratings):.1f}\")\n",
        "    print(f\"  Mode: {pd.Series(readability_ratings).mode().values}\")\n",
        "    print(f\"  Distribution: {read_counts.to_dict()}\")\n",
        "\n",
        "    # Create visualizations based on the requested plot type\n",
        "    if plot_type == 'count' or plot_type == 'all':\n",
        "        # Count Plot\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        ax = sns.countplot(x='Rating Value', hue='Category', data=long_df)\n",
        "        plt.title(f'Distribution of Rating Scores{title_suffix}')\n",
        "        plt.xlabel('Rating (1-4 scale)')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        # Add count labels on top of bars\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            if height > 0:\n",
        "                ax.text(p.get_x() + p.get_width()/2.,\n",
        "                        height + 0.1,\n",
        "                        f'{height:.0f}',\n",
        "                        ha=\"center\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    if plot_type == 'box' or plot_type == 'all':\n",
        "        # Box Plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        ax = sns.boxplot(x='Category', y='Rating Value', data=long_df)\n",
        "        plt.title(f'Box Plot of Rating Distributions{title_suffix}')\n",
        "        plt.ylabel('Rating (1-4 scale)')\n",
        "\n",
        "        # Add jittered data points for better visibility\n",
        "        sns.stripplot(x='Category', y='Rating Value', data=long_df,\n",
        "                      color='black', alpha=0.5, jitter=True)\n",
        "\n",
        "        # Set y-axis to show only the possible rating values\n",
        "        plt.yticks([1, 2, 3, 4])\n",
        "        plt.ylim(0.5, 4.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    if plot_type == 'grouped' or plot_type == 'all':\n",
        "        # Grouped Bar Chart\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Convert to percentages for easier comparison\n",
        "        ratings_pct = ratings_df.copy()\n",
        "        corr_total = sum(corr_counts)\n",
        "        read_total = sum(read_counts)\n",
        "\n",
        "        for i, row in ratings_pct.iterrows():\n",
        "            if row['Category'] == 'Correspondence':\n",
        "                ratings_pct.at[i, 'Percentage'] = (row['Count'] / corr_total * 100) if corr_total > 0 else 0\n",
        "            else:\n",
        "                ratings_pct.at[i, 'Percentage'] = (row['Count'] / read_total * 100) if read_total > 0 else 0\n",
        "\n",
        "        # Create the grouped bar chart\n",
        "        ax = sns.barplot(x='Rating Value', y='Percentage', hue='Category', data=ratings_pct)\n",
        "        plt.title(f'Percentage of Ratings by Category{title_suffix}')\n",
        "        plt.xlabel('Rating (1-4 scale)')\n",
        "        plt.ylabel('Percentage (%)')\n",
        "\n",
        "        # Add percentage labels on top of bars\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            if height > 0:\n",
        "                ax.text(p.get_x() + p.get_width()/2.,\n",
        "                        height + 1,\n",
        "                        f'{height:.1f}%',\n",
        "                        ha=\"center\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    if plot_type == 'all':\n",
        "        # Combined visualization with all plots\n",
        "        fig = plt.figure(figsize=(18, 10))\n",
        "        gs = GridSpec(2, 2, figure=fig)\n",
        "\n",
        "        # Count plot\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        sns.countplot(x='Rating Value', hue='Category', data=long_df, ax=ax1)\n",
        "        ax1.set_title('Distribution of Rating Scores')\n",
        "        ax1.set_xlabel('Rating (1-4 scale)')\n",
        "        ax1.set_ylabel('Count')\n",
        "\n",
        "        # Box plot\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        sns.boxplot(x='Category', y='Rating Value', data=long_df, ax=ax2)\n",
        "        sns.stripplot(x='Category', y='Rating Value', data=long_df,\n",
        "                      color='black', alpha=0.5, jitter=True, ax=ax2)\n",
        "        ax2.set_title('Box Plot of Rating Distributions')\n",
        "        ax2.set_ylabel('Rating (1-4 scale)')\n",
        "        ax2.set_yticks([1, 2, 3, 4])\n",
        "        ax2.set_ylim(0.5, 4.5)\n",
        "\n",
        "        # Grouped bar chart\n",
        "        ax3 = fig.add_subplot(gs[1, :])\n",
        "        sns.barplot(x='Rating Value', y='Percentage', hue='Category', data=ratings_pct, ax=ax3)\n",
        "        ax3.set_title('Percentage of Ratings by Category')\n",
        "        ax3.set_xlabel('Rating (1-4 scale)')\n",
        "        ax3.set_ylabel('Percentage (%)')\n",
        "\n",
        "        fig.suptitle(f'Rating Analysis{title_suffix}', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.subplots_adjust(top=0.9)\n",
        "        plt.show()\n",
        "\n",
        "    return ratings_df, long_df"
      ],
      "metadata": {
        "id": "hxAB1rf6rnFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall correspondence and readability scores - Visualizations"
      ],
      "metadata": {
        "id": "uaFfzVb7tg29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All visualizations"
      ],
      "metadata": {
        "id": "rO7P3LV6tl5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all plot types for a specific document\n",
        "visualize_rating_distributions(processed_data, doc_id=1, plot_type='all');"
      ],
      "metadata": {
        "id": "2atoOmOtrqYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Count plot on rating distributions"
      ],
      "metadata": {
        "id": "fesOH-SltTIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Interpret Rating Distribution Charts**\n",
        "\n",
        "This chart visualizes the distribution of translation quality ratings across two assessment categories: correspondence and readability.\n",
        "\n",
        "*Reading the Count Plot*\n",
        "- X-axis: Shows the rating scale (1-4), where 1 is lowest quality and 4 is highest quality\n",
        "- Y-axis: Shows the count (number of annotators) who assigned each rating\n",
        "- Color: Distinguishes between Readability (blue) and Correspondence (orange) ratings\n",
        "\n",
        "*Key Insights*\n",
        "- Rating Distribution: Taller bars indicate more common ratings\n",
        "- Rating Patterns: Compare blue vs. orange bars to see differences between how annotators rated readability vs. correspondence\n",
        "- Agreement Level: Bars concentrated at one rating value suggest stronger annotator agreement\n",
        "- Quality Assessment: The concentration of ratings at higher numbers (3-4) indicates better perceived translation quality\n",
        "\n",
        "*Document selection*\n",
        "\n",
        "This visualization aggregates ratings across all documents. To view ratings for a specific document, use the document ID parameter when calling the visualization function.\n",
        "\n",
        "Show only count plot for a single documents: visualize_rating_distributions(processed_data, doc_id=1, plot_type='count');"
      ],
      "metadata": {
        "id": "TJWRZxdgtsxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show only count plot for all documents\n",
        "visualize_rating_distributions(processed_data, doc_id=None, plot_type='count');"
      ],
      "metadata": {
        "id": "WTFiVC_VsbFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Box plot on rating distribution"
      ],
      "metadata": {
        "id": "FJBYz3vNuVc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show only box plot for document ID 2\n",
        "visualize_rating_distributions(processed_data, doc_id=2, plot_type='box');"
      ],
      "metadata": {
        "id": "H9wSsBLvujlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Grouped bar chart of rating distributions"
      ],
      "metadata": {
        "id": "tlkykPtBuuf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show only grouped bar chart for document ID 3\n",
        "visualize_rating_distributions(processed_data, doc_id=3, plot_type='grouped');"
      ],
      "metadata": {
        "id": "zo7jMFDVuzxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Translation Error Annotations - Agreement Calculations\n",
        "\n",
        "- Exact matching for span boundaries\n",
        "- F1 for partial credit on spans\n",
        "- Kappa for category agreement, subcategory agreement, and severity levels"
      ],
      "metadata": {
        "id": "Yde2AK9mhEGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Exact Matching for Span Boundaries\n"
      ],
      "metadata": {
        "id": "BiAbqvfC4bNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How it works**\n",
        "- Measures when two or more annotators identify exactly the same text span (identical start and end positions)\n",
        "- Typically expressed as a percentage: \"Annotators agreed on exact span boundaries for 65% of errors\"\n",
        "\n",
        "**What you'll learn**\n",
        "- How precisely annotators agree on where errors begin and end\n",
        "- Identifies potential ambiguity in annotation guidelines\n",
        "- Shows if annotators are consistently identifying the same textual issues\n",
        "\n",
        "In MQM translation error annotation context, exact matching shows how often annotators precisely agree on which specific words or phrases contain errors."
      ],
      "metadata": {
        "id": "qoRjoUvu7imT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exact Matching for Span Boundaries - Code**"
      ],
      "metadata": {
        "id": "zGs52hZe5suv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_exact_matches(doc_id_to_annotations):\n",
        "    \"\"\"\n",
        "    Calculate exact matches between annotators for each document.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with exact match statistics\n",
        "    \"\"\"\n",
        "    # Results structure\n",
        "    results = {\n",
        "        'per_document': {},\n",
        "        'overall': {\n",
        "            'total_comparisons': 0,\n",
        "            'total_exact_matches': 0,\n",
        "            'exact_match_rate': 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Process each document\n",
        "    for doc_id, annotations in doc_id_to_annotations.items():\n",
        "        # Group annotations by annotator\n",
        "        annotator_to_spans = {}\n",
        "\n",
        "        for annotation in annotations:\n",
        "            annotator = annotation.get('annotator')\n",
        "            if not annotator:\n",
        "                continue\n",
        "\n",
        "            # Get all span boundaries for this annotator\n",
        "            spans = []\n",
        "            for label in annotation.get('enriched_labels', []):\n",
        "                spans.append({\n",
        "                    'start': label.get('start'),\n",
        "                    'end': label.get('end'),\n",
        "                    'text': label.get('text'),\n",
        "                    'category': label.get('labels', [''])[0] if label.get('labels') else '',\n",
        "                    'subcategory': label.get('subcategory', ''),\n",
        "                    'severity': label.get('severity', '')\n",
        "                })\n",
        "\n",
        "            annotator_to_spans[annotator] = spans\n",
        "\n",
        "        # Skip documents with fewer than 2 annotators\n",
        "        if len(annotator_to_spans) < 2:\n",
        "            continue\n",
        "\n",
        "        # Initialize document results\n",
        "        doc_results = {\n",
        "            'annotator_pairs': [],\n",
        "            'total_comparisons': 0,\n",
        "            'total_exact_matches': 0,\n",
        "            'exact_match_rate': 0\n",
        "        }\n",
        "\n",
        "        # Compare each pair of annotators\n",
        "        for annotator1, annotator2 in combinations(annotator_to_spans.keys(), 2):\n",
        "            spans1 = annotator_to_spans[annotator1]\n",
        "            spans2 = annotator_to_spans[annotator2]\n",
        "\n",
        "            # Count exact matches\n",
        "            exact_matches = 0\n",
        "\n",
        "            # Create sets of (start, end) tuples for each annotator\n",
        "            spans1_set = {(span['start'], span['end']) for span in spans1}\n",
        "            spans2_set = {(span['start'], span['end']) for span in spans2}\n",
        "\n",
        "            # Find intersection (exact matches)\n",
        "            exact_match_set = spans1_set.intersection(spans2_set)\n",
        "            exact_matches = len(exact_match_set)\n",
        "\n",
        "            # Calculate comparison stats\n",
        "            total_spans = len(spans1) + len(spans2)\n",
        "            unique_spans = len(spans1_set.union(spans2_set))\n",
        "\n",
        "            # Avoid division by zero\n",
        "            exact_match_rate = exact_matches / unique_spans if unique_spans > 0 else 0\n",
        "\n",
        "            # Store pair results\n",
        "            pair_result = {\n",
        "                'annotator1': annotator1,\n",
        "                'annotator2': annotator2,\n",
        "                'spans1': len(spans1),\n",
        "                'spans2': len(spans2),\n",
        "                'unique_spans': unique_spans,\n",
        "                'exact_matches': exact_matches,\n",
        "                'exact_match_rate': exact_match_rate\n",
        "            }\n",
        "\n",
        "            doc_results['annotator_pairs'].append(pair_result)\n",
        "            doc_results['total_comparisons'] += unique_spans\n",
        "            doc_results['total_exact_matches'] += exact_matches\n",
        "\n",
        "        # Calculate overall document exact match rate\n",
        "        if doc_results['total_comparisons'] > 0:\n",
        "            doc_results['exact_match_rate'] = doc_results['total_exact_matches'] / doc_results['total_comparisons']\n",
        "\n",
        "        # Store document results\n",
        "        results['per_document'][doc_id] = doc_results\n",
        "\n",
        "        # Update overall statistics\n",
        "        results['overall']['total_comparisons'] += doc_results['total_comparisons']\n",
        "        results['overall']['total_exact_matches'] += doc_results['total_exact_matches']\n",
        "\n",
        "    # Calculate overall exact match rate\n",
        "    if results['overall']['total_comparisons'] > 0:\n",
        "        results['overall']['exact_match_rate'] = results['overall']['total_exact_matches'] / results['overall']['total_comparisons']\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "QjmBJgdghShJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_exact_match_results(exact_match_results):\n",
        "    \"\"\"\n",
        "    Display exact match results in a readable format with visualizations.\n",
        "\n",
        "    Args:\n",
        "        exact_match_results: Output from calculate_exact_matches function\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"EXACT MATCH ANALYSIS FOR SPAN BOUNDARIES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Overall results\n",
        "    overall = exact_match_results['overall']\n",
        "    print(f\"\\nOVERALL RESULTS:\")\n",
        "    print(f\"Total comparisons: {overall['total_comparisons']}\")\n",
        "    print(f\"Total exact matches: {overall['total_exact_matches']}\")\n",
        "    print(f\"Exact match rate: {overall['exact_match_rate']:.2%}\")\n",
        "\n",
        "    # Per-document results\n",
        "    print(\"\\nRESULTS BY DOCUMENT:\")\n",
        "    for doc_id, doc_results in exact_match_results['per_document'].items():\n",
        "        print(f\"\\nDocument {doc_id}:\")\n",
        "        print(f\"  Exact match rate: {doc_results['exact_match_rate']:.2%}\")\n",
        "        print(f\"  Exact matches: {doc_results['total_exact_matches']} out of {doc_results['total_comparisons']}\")\n",
        "\n",
        "        # Results by annotator pair\n",
        "        print(\"\\n  Results by annotator pair:\")\n",
        "        for pair in doc_results['annotator_pairs']:\n",
        "            print(f\"    Annotators {pair['annotator1']} and {pair['annotator2']}:\")\n",
        "            print(f\"      Spans: {pair['spans1']} and {pair['spans2']}\")\n",
        "            print(f\"      Exact matches: {pair['exact_matches']} out of {pair['unique_spans']} unique spans\")\n",
        "            print(f\"      Exact match rate: {pair['exact_match_rate']:.2%}\")\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Prepare data for bar chart\n",
        "    doc_ids = []\n",
        "    doc_rates = []\n",
        "\n",
        "    for doc_id, doc_results in exact_match_results['per_document'].items():\n",
        "        doc_ids.append(f\"Doc {doc_id}\")\n",
        "        doc_rates.append(doc_results['exact_match_rate'] * 100)\n",
        "\n",
        "    # Add overall rate\n",
        "    doc_ids.append(\"Overall\")\n",
        "    doc_rates.append(overall['exact_match_rate'] * 100)\n",
        "\n",
        "    # Plot\n",
        "    bars = plt.bar(doc_ids, doc_rates, color='skyblue')\n",
        "    plt.axhline(y=overall['exact_match_rate'] * 100, color='r', linestyle='--', alpha=0.7, label='Overall Average')\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('Exact Match Rate by Document')\n",
        "    plt.xlabel('Document')\n",
        "    plt.ylabel('Exact Match Rate (%)')\n",
        "    plt.ylim(0, max(doc_rates) * 1.2)  # Add some space for labels\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O5JuP0cP5qHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate exact matches\n",
        "exact_match_results = calculate_exact_matches(doc_id_to_annotations)"
      ],
      "metadata": {
        "id": "7gFVpFRT55DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exact Matching for Span Boundaries - Visualizations**"
      ],
      "metadata": {
        "id": "ICu3ZDpj527W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "display_exact_match_results(exact_match_results)"
      ],
      "metadata": {
        "id": "koeSJavC59la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 F1 for Partial Credit on Spans"
      ],
      "metadata": {
        "id": "elWRm2Tp4vo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How F1 Calculation Works**\n",
        "\n",
        "The core concept: Instead of requiring exact span boundaries to match, we:\n",
        "- Give credit when spans overlap\n",
        "- Calculate how much they overlap relative to each annotator's span (precision and recall)\n",
        "- Combine these into an F1 score\n",
        "- Values range from 0 (no overlap) to 1 (perfect overlap)\n",
        "\n",
        "For each span:\n",
        "- Precision: What percentage of annotator 1's span is overlapped by annotator 2's span\n",
        "- Recall: What percentage of annotator 2's span is overlapped by annotator 1's span\n",
        "- F1: The harmonic mean of precision and recall (balances both)\n",
        "\n",
        "Matching process:\n",
        "- For each span from annotator 1, find the span from annotator 2 that gives the highest F1 score\n",
        "- If multiple spans from annotator 2 overlap with a span from annotator 1, only the best match counts\n",
        "\n",
        "Aggregation:\n",
        "- Calculate average precision, recall, and F1 for each annotator pair\n",
        "- Calculate averages per document and overall across all documents\n",
        "\n",
        "**What You'll Learn From F1 Scores**\n",
        "\n",
        "F1 scores give a more nuanced view of agreement than exact matching. For example, if one annotator marks \"aerial carbon\" as an error while another marks \"aerial carbon in trees\", F1 would give partial credit for this overlap.\n",
        "\n",
        "F1 scores are likely be much higher than exact match percentages, showing that:\n",
        "- Annotators are often identifying the same errors but choosing slightly different span boundaries\n",
        "- Some annotator pairs have more similar annotation styles than others\n",
        "- Some documents may have more clearly defined errors, leading to higher agreement\n",
        "\n",
        "The F1 visualization will help you identify which annotators tend to agree more with others, and which documents have the most consistent annotations."
      ],
      "metadata": {
        "id": "MWWqZYL88akf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **F1 for Partial Credit on Spans - Code**"
      ],
      "metadata": {
        "id": "KiMdYuuw8CT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_span_f1(doc_id_to_annotations, overlap_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Calculate F1 scores for partial span matches between annotators.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "        overlap_threshold: Minimum overlap ratio required to consider spans as matching (0.0 means any overlap)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with F1 statistics\n",
        "    \"\"\"\n",
        "    # Results structure\n",
        "    results = {\n",
        "        'per_document': {},\n",
        "        'overall': {\n",
        "            'total_comparisons': 0,\n",
        "            'avg_precision': 0,\n",
        "            'avg_recall': 0,\n",
        "            'avg_f1': 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    overall_precisions = []\n",
        "    overall_recalls = []\n",
        "    overall_f1s = []\n",
        "\n",
        "    # Process each document\n",
        "    for doc_id, annotations in doc_id_to_annotations.items():\n",
        "        # Group annotations by annotator\n",
        "        annotator_to_spans = {}\n",
        "\n",
        "        for annotation in annotations:\n",
        "            annotator = annotation.get('annotator')\n",
        "            if not annotator:\n",
        "                continue\n",
        "\n",
        "            # Get all span boundaries for this annotator\n",
        "            spans = []\n",
        "            for label in annotation.get('enriched_labels', []):\n",
        "                spans.append({\n",
        "                    'start': label.get('start'),\n",
        "                    'end': label.get('end'),\n",
        "                    'text': label.get('text'),\n",
        "                    'length': label.get('end') - label.get('start'),\n",
        "                    'category': label.get('labels', [''])[0] if label.get('labels') else '',\n",
        "                    'subcategory': label.get('subcategory', ''),\n",
        "                    'severity': label.get('severity', '')\n",
        "                })\n",
        "\n",
        "            annotator_to_spans[annotator] = spans\n",
        "\n",
        "        # Skip documents with fewer than 2 annotators\n",
        "        if len(annotator_to_spans) < 2:\n",
        "            continue\n",
        "\n",
        "        # Initialize document results\n",
        "        doc_results = {\n",
        "            'annotator_pairs': [],\n",
        "            'avg_precision': 0,\n",
        "            'avg_recall': 0,\n",
        "            'avg_f1': 0\n",
        "        }\n",
        "\n",
        "        doc_precisions = []\n",
        "        doc_recalls = []\n",
        "        doc_f1s = []\n",
        "\n",
        "        # Compare each pair of annotators\n",
        "        for annotator1, annotator2 in combinations(annotator_to_spans.keys(), 2):\n",
        "            spans1 = annotator_to_spans[annotator1]\n",
        "            spans2 = annotator_to_spans[annotator2]\n",
        "\n",
        "            # Skip if either annotator has no spans\n",
        "            if not spans1 or not spans2:\n",
        "                continue\n",
        "\n",
        "            # Create binary arrays representing each character position\n",
        "            # This is a more rigorous way to calculate true precision/recall\n",
        "            max_pos = max([span['end'] for spans in [spans1, spans2] for span in spans])\n",
        "            array1 = np.zeros(max_pos + 1, dtype=bool)\n",
        "            array2 = np.zeros(max_pos + 1, dtype=bool)\n",
        "\n",
        "            # Mark positions covered by each annotator's spans\n",
        "            for span in spans1:\n",
        "                array1[span['start']:span['end']] = True\n",
        "            for span in spans2:\n",
        "                array2[span['start']:span['end']] = True\n",
        "\n",
        "            # Calculate true precision and recall using the character arrays\n",
        "            # Precision: proportion of characters marked by annotator1 that are also marked by annotator2\n",
        "            # Recall: proportion of characters marked by annotator2 that are also marked by annotator1\n",
        "            true_positives = np.sum(array1 & array2)\n",
        "            total_predicted = np.sum(array1)\n",
        "            total_actual = np.sum(array2)\n",
        "\n",
        "            precision = true_positives / total_predicted if total_predicted > 0 else 0\n",
        "            recall = true_positives / total_actual if total_actual > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            # Track match statistics for reporting purposes\n",
        "            # Find overlapping spans for detailed reporting\n",
        "            matches = []\n",
        "            for span1 in spans1:\n",
        "                for span2 in spans2:\n",
        "                    overlap_start = max(span1['start'], span2['start'])\n",
        "                    overlap_end = min(span1['end'], span2['end'])\n",
        "\n",
        "                    if overlap_start < overlap_end:  # Spans overlap\n",
        "                        matches.append({\n",
        "                            'span1': span1,\n",
        "                            'span2': span2,\n",
        "                            'overlap_length': overlap_end - overlap_start\n",
        "                        })\n",
        "\n",
        "            # Store pair results\n",
        "            pair_result = {\n",
        "                'annotator1': annotator1,\n",
        "                'annotator2': annotator2,\n",
        "                'spans1': len(spans1),\n",
        "                'spans2': len(spans2),\n",
        "                'overlap_matches': len(matches),\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "            doc_precisions.append(precision)\n",
        "            doc_recalls.append(recall)\n",
        "            doc_f1s.append(f1)\n",
        "\n",
        "            overall_precisions.append(precision)\n",
        "            overall_recalls.append(recall)\n",
        "            overall_f1s.append(f1)\n",
        "\n",
        "            doc_results['annotator_pairs'].append(pair_result)\n",
        "\n",
        "        # Calculate document averages\n",
        "        if doc_precisions:\n",
        "            doc_results['avg_precision'] = sum(doc_precisions) / len(doc_precisions)\n",
        "            doc_results['avg_recall'] = sum(doc_recalls) / len(doc_recalls)\n",
        "            doc_results['avg_f1'] = sum(doc_f1s) / len(doc_f1s)\n",
        "\n",
        "        # Store document results\n",
        "        results['per_document'][doc_id] = doc_results\n",
        "\n",
        "    # Calculate overall averages\n",
        "    if overall_precisions:\n",
        "        results['overall']['avg_precision'] = sum(overall_precisions) / len(overall_precisions)\n",
        "        results['overall']['avg_recall'] = sum(overall_recalls) / len(overall_recalls)\n",
        "        results['overall']['avg_f1'] = sum(overall_f1s) / len(overall_f1s)\n",
        "        results['overall']['total_comparisons'] = len(overall_f1s)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Qo5pBKT0414h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_span_f1_results(f1_results):\n",
        "    \"\"\"\n",
        "    Display F1 results in a readable format with visualizations.\n",
        "\n",
        "    Args:\n",
        "        f1_results: Output from calculate_span_f1 function\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"F1 ANALYSIS FOR PARTIAL SPAN MATCHES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Overall results\n",
        "    overall = f1_results['overall']\n",
        "    print(f\"\\nOVERALL RESULTS:\")\n",
        "    print(f\"Total annotator pair comparisons: {overall['total_comparisons']}\")\n",
        "    print(f\"Average Precision: {overall['avg_precision']:.2%}\")\n",
        "    print(f\"Average Recall: {overall['avg_recall']:.2%}\")\n",
        "    print(f\"Average F1 Score: {overall['avg_f1']:.2%}\")\n",
        "\n",
        "    # Per-document results\n",
        "    print(\"\\nRESULTS BY DOCUMENT:\")\n",
        "    for doc_id, doc_results in f1_results['per_document'].items():\n",
        "        print(f\"\\nDocument {doc_id}:\")\n",
        "        print(f\"  Average F1 Score: {doc_results['avg_f1']:.2%}\")\n",
        "        print(f\"  Average Precision: {doc_results['avg_precision']:.2%}\")\n",
        "        print(f\"  Average Recall: {doc_results['avg_recall']:.2%}\")\n",
        "\n",
        "        # Results by annotator pair\n",
        "        print(\"\\n  Results by annotator pair:\")\n",
        "        for pair in sorted(doc_results['annotator_pairs'], key=lambda x: x['f1'], reverse=True):\n",
        "            print(f\"    Annotators {pair['annotator1']} and {pair['annotator2']}:\")\n",
        "            print(f\"      Spans: {pair['spans1']} and {pair['spans2']}\")\n",
        "            print(f\"      Overlapping spans: {pair['overlap_matches']}\")  # Changed from 'matches' to 'overlap_matches'\n",
        "            print(f\"      Precision: {pair['precision']:.2%}\")\n",
        "            print(f\"      Recall: {pair['recall']:.2%}\")\n",
        "            print(f\"      F1 Score: {pair['f1']:.2%}\")\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Prepare data for bar chart\n",
        "    doc_ids = []\n",
        "    doc_f1s = []\n",
        "    doc_precisions = []\n",
        "    doc_recalls = []\n",
        "\n",
        "    for doc_id, doc_results in f1_results['per_document'].items():\n",
        "        doc_ids.append(f\"Doc {doc_id}\")\n",
        "        doc_f1s.append(doc_results['avg_f1'] * 100)\n",
        "        doc_precisions.append(doc_results['avg_precision'] * 100)\n",
        "        doc_recalls.append(doc_results['avg_recall'] * 100)\n",
        "\n",
        "    # Add overall scores\n",
        "    doc_ids.append(\"Overall\")\n",
        "    doc_f1s.append(overall['avg_f1'] * 100)\n",
        "    doc_precisions.append(overall['avg_precision'] * 100)\n",
        "    doc_recalls.append(overall['avg_recall'] * 100)\n",
        "\n",
        "    # Set bar width\n",
        "    barWidth = 0.25\n",
        "\n",
        "    # Set position of bars on X axis\n",
        "    r1 = np.arange(len(doc_ids))\n",
        "    r2 = [x + barWidth for x in r1]\n",
        "    r3 = [x + barWidth for x in r2]\n",
        "\n",
        "    # Create grouped bars\n",
        "    plt.bar(r1, doc_f1s, width=barWidth, color='blue', label='F1 Score')\n",
        "    plt.bar(r2, doc_precisions, width=barWidth, color='green', label='Precision')\n",
        "    plt.bar(r3, doc_recalls, width=barWidth, color='orange', label='Recall')\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.xlabel('Document')\n",
        "    plt.ylabel('Score (%)')\n",
        "    plt.title('F1, Precision, and Recall by Document')\n",
        "    plt.xticks([r + barWidth for r in range(len(doc_ids))], doc_ids)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eIWzfm5l9wzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate F1 scores for partial matches\n",
        "# Any overlap is considered a match (threshold=0.0)\n",
        "f1_results = calculate_span_f1(doc_id_to_annotations, overlap_threshold=0.0)"
      ],
      "metadata": {
        "id": "Pp3MEPs496a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **F1 for Partial Credit on Spans - Visualizations**"
      ],
      "metadata": {
        "id": "rYZBYPq98H7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "display_span_f1_results(f1_results)"
      ],
      "metadata": {
        "id": "gYhfBGc798xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Kappa for Category Agreement\n",
        "\n",
        "**How it works**\n",
        "- Cohen's Kappa (two annotators) or Fleiss' Kappa (multiple annotators) measures agreement on categories while accounting for chance agreement\n",
        "- Scale typically ranges from -1 to 1:\n",
        " - < 0.20: Poor agreement\n",
        " - 0.21-0.40: Fair agreement\n",
        " - 0.41-0.60: Moderate agreement\n",
        " - 0.61-0.80: Substantial agreement\n",
        " - 0.81-1.00: Almost perfect agreement\n",
        "\n",
        "**What you could potentially learn**\n",
        "- How consistently annotators classify errors into categories (Accuracy, Terminology, Style)\n",
        "- How consistently annotators use subcategories (e.g., \"TERM: Wrong term\")\n",
        "- How consistently annotators assign severity levels (Minor vs. Major)\n",
        "- Whether certain categories have better agreement than others\n",
        "\n",
        "This is particularly valuable for the MQM framework since it has a complex hierarchy of error types.\n",
        "\n",
        "**Challenges inherent in this approach**\n",
        "- Low exact match rates and F1 scores present serious challenges for calculating meaningful Kappa agreement\n",
        "- There will be even lower agreement as we transition from each level of agreement to the next:\n",
        " - Span agreement\n",
        " - Label agreement\n",
        " - Subcategory agreement\n",
        " - Severity agreement\n",
        "\n",
        "**Parameters implemented to respond to challenges**\n",
        "- Finding spans with substantial overlap (≥50%)\n",
        "- Only calculating Kappa for these overlapping spans\n",
        "\n",
        "**Recommendations**\n",
        "\n",
        "Where there are low exact match rates, F1 scores, and Kappa scores for category agreement, proceeding to Kappa scoring for subcategory agreement and severity agreement is not recommended."
      ],
      "metadata": {
        "id": "fo_qMLV_5BiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Kappa for Category Agreement - Code**"
      ],
      "metadata": {
        "id": "FC8u3WgWB-_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_category_agreement(doc_id_to_annotations, attribute='labels'):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa for agreement on categories, subcategories, or severity levels\n",
        "    across annotator pairs.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "        attribute: The attribute to measure agreement on ('labels', 'subcategory', or 'severity')\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with agreement statistics\n",
        "    \"\"\"\n",
        "    # Results structure\n",
        "    results = {\n",
        "        'per_document': {},\n",
        "        'overall': {\n",
        "            'avg_kappa': 0,\n",
        "            'category_counts': {},\n",
        "            'pair_count': 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Overall statistics\n",
        "    all_kappas = []\n",
        "    all_weights = []\n",
        "    all_category_counts = {}\n",
        "\n",
        "    # Process each document\n",
        "    for doc_id, annotations in doc_id_to_annotations.items():\n",
        "        # Group annotations by annotator\n",
        "        annotator_to_spans = {}\n",
        "\n",
        "        for annotation in annotations:\n",
        "            annotator = annotation.get('annotator')\n",
        "            if not annotator:\n",
        "                continue\n",
        "\n",
        "            # Extract spans for this annotator\n",
        "            spans = []\n",
        "            for label in annotation.get('enriched_labels', []):\n",
        "                span = {\n",
        "                    'start': label.get('start'),\n",
        "                    'end': label.get('end'),\n",
        "                    'text': label.get('text')\n",
        "                }\n",
        "\n",
        "                # Extract the relevant attribute value\n",
        "                if attribute == 'labels':\n",
        "                    # For labels, take the first item from the list\n",
        "                    span['category'] = label.get('labels', [''])[0] if label.get('labels') else ''\n",
        "                elif attribute == 'subcategory':\n",
        "                    span['category'] = label.get('subcategory', '')\n",
        "                elif attribute == 'severity':\n",
        "                    span['category'] = label.get('severity', '')\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                spans.append(span)\n",
        "\n",
        "            # Store spans for this annotator\n",
        "            annotator_to_spans[annotator] = spans\n",
        "\n",
        "        # Skip documents with fewer than 2 annotators\n",
        "        if len(annotator_to_spans) < 2:\n",
        "            continue\n",
        "\n",
        "        # Initialize document results\n",
        "        doc_results = {\n",
        "            'annotator_pairs': [],\n",
        "            'avg_kappa': 0,\n",
        "            'category_counts': {},\n",
        "            'pair_count': 0\n",
        "        }\n",
        "\n",
        "        # Calculate agreement for each pair of annotators\n",
        "        doc_kappas = []\n",
        "        doc_weights = []\n",
        "        doc_category_counts = {}\n",
        "\n",
        "        for annotator1, annotator2 in combinations(annotator_to_spans.keys(), 2):\n",
        "            spans1 = annotator_to_spans[annotator1]\n",
        "            spans2 = annotator_to_spans[annotator2]\n",
        "\n",
        "            # Find overlapping spans\n",
        "            overlapping_spans = []\n",
        "\n",
        "            for span1 in spans1:\n",
        "                for span2 in spans2:\n",
        "                    # Check for overlap\n",
        "                    if span1['start'] < span2['end'] and span2['start'] < span1['end']:\n",
        "                        # Calculate overlap\n",
        "                        overlap_start = max(span1['start'], span2['start'])\n",
        "                        overlap_end = min(span1['end'], span2['end'])\n",
        "                        overlap_len = overlap_end - overlap_start\n",
        "\n",
        "                        # Calculate overlap ratio relative to the smaller span\n",
        "                        span1_len = span1['end'] - span1['start']\n",
        "                        span2_len = span2['end'] - span2['start']\n",
        "                        min_len = min(span1_len, span2_len)\n",
        "                        overlap_ratio = overlap_len / min_len if min_len > 0 else 0\n",
        "\n",
        "                        # Only consider substantial overlaps (at least 50%)\n",
        "                        if overlap_ratio >= 0.5:\n",
        "                            overlapping_spans.append({\n",
        "                                'span1': span1,\n",
        "                                'span2': span2,\n",
        "                                'overlap_ratio': overlap_ratio\n",
        "                            })\n",
        "\n",
        "            # Skip pairs with too few overlapping spans for Kappa\n",
        "            if len(overlapping_spans) < 2:\n",
        "                continue\n",
        "\n",
        "            # Extract categories for overlapping spans\n",
        "            categories1 = [overlap['span1']['category'] for overlap in overlapping_spans]\n",
        "            categories2 = [overlap['span2']['category'] for overlap in overlapping_spans]\n",
        "\n",
        "            # Count category frequencies\n",
        "            for category in categories1 + categories2:\n",
        "                doc_category_counts[category] = doc_category_counts.get(category, 0) + 1\n",
        "                all_category_counts[category] = all_category_counts.get(category, 0) + 1\n",
        "\n",
        "            # Calculate Cohen's Kappa\n",
        "            try:\n",
        "                kappa = cohen_kappa_score(categories1, categories2)\n",
        "                weight = len(overlapping_spans)\n",
        "\n",
        "                doc_kappas.append(kappa)\n",
        "                doc_weights.append(weight)\n",
        "\n",
        "                all_kappas.append(kappa)\n",
        "                all_weights.append(weight)\n",
        "\n",
        "                # Record pair results\n",
        "                pair_result = {\n",
        "                    'annotator1': annotator1,\n",
        "                    'annotator2': annotator2,\n",
        "                    'overlapping_spans': len(overlapping_spans),\n",
        "                    'kappa': kappa\n",
        "                }\n",
        "\n",
        "                doc_results['annotator_pairs'].append(pair_result)\n",
        "                doc_results['pair_count'] += 1\n",
        "            except Exception as e:\n",
        "                # Skip pairs with invalid Kappa calculation\n",
        "                print(f\"Warning: Could not calculate Kappa for annotators {annotator1} and {annotator2} - {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate weighted average Kappa for this document\n",
        "        if doc_kappas:\n",
        "            total_weight = sum(doc_weights)\n",
        "            doc_results['avg_kappa'] = sum(k * w for k, w in zip(doc_kappas, doc_weights)) / total_weight\n",
        "            doc_results['category_counts'] = doc_category_counts\n",
        "\n",
        "        # Store document results\n",
        "        results['per_document'][doc_id] = doc_results\n",
        "\n",
        "    # Calculate overall weighted average Kappa\n",
        "    if all_kappas:\n",
        "        total_weight = sum(all_weights)\n",
        "        results['overall']['avg_kappa'] = sum(k * w for k, w in zip(all_kappas, all_weights)) / total_weight\n",
        "        results['overall']['category_counts'] = all_category_counts\n",
        "        results['overall']['pair_count'] = len(all_kappas)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "zdlqaff95CYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_category_agreement(agreement_results, attribute_name):\n",
        "    \"\"\"\n",
        "    Display category agreement results in a readable format with visualizations.\n",
        "\n",
        "    Args:\n",
        "        agreement_results: Output from calculate_category_agreement function\n",
        "        attribute_name: Name of the attribute (e.g., 'Category', 'Subcategory', 'Severity')\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"COHEN'S KAPPA ANALYSIS FOR {attribute_name.upper()} AGREEMENT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Overall results\n",
        "    overall = agreement_results['overall']\n",
        "    print(f\"\\nOVERALL RESULTS:\")\n",
        "\n",
        "    # Interpret Kappa value\n",
        "    kappa = overall['avg_kappa']\n",
        "    interpretation = \"Poor agreement\"\n",
        "    if kappa > 0.80:\n",
        "        interpretation = \"Almost perfect agreement\"\n",
        "    elif kappa > 0.60:\n",
        "        interpretation = \"Substantial agreement\"\n",
        "    elif kappa > 0.40:\n",
        "        interpretation = \"Moderate agreement\"\n",
        "    elif kappa > 0.20:\n",
        "        interpretation = \"Fair agreement\"\n",
        "\n",
        "    print(f\"Average Cohen's Kappa: {kappa:.4f} - {interpretation}\")\n",
        "    print(f\"Total annotator pairs analyzed: {overall['pair_count']}\")\n",
        "\n",
        "    # Category-specific results\n",
        "    print(f\"\\n{attribute_name.upper()} FREQUENCY:\")\n",
        "    total_count = sum(overall['category_counts'].values())\n",
        "\n",
        "    # Sort categories by frequency\n",
        "    sorted_categories = sorted(overall['category_counts'].items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for category, count in sorted_categories:\n",
        "        frequency = count / total_count if total_count > 0 else 0\n",
        "        print(f\"  {category}: {frequency:.2%}\")\n",
        "\n",
        "    # Per-document results\n",
        "    print(\"\\nRESULTS BY DOCUMENT:\")\n",
        "    for doc_id, doc_results in agreement_results['per_document'].items():\n",
        "        doc_kappa = doc_results['avg_kappa']\n",
        "        doc_interpretation = \"Poor agreement\"\n",
        "        if doc_kappa > 0.80:\n",
        "            doc_interpretation = \"Almost perfect agreement\"\n",
        "        elif doc_kappa > 0.60:\n",
        "            doc_interpretation = \"Substantial agreement\"\n",
        "        elif doc_kappa > 0.40:\n",
        "            doc_interpretation = \"Moderate agreement\"\n",
        "        elif doc_kappa > 0.20:\n",
        "            doc_interpretation = \"Fair agreement\"\n",
        "\n",
        "        print(f\"\\nDocument {doc_id}:\")\n",
        "        print(f\"  Kappa: {doc_kappa:.4f} - {doc_interpretation}\")\n",
        "        print(f\"  Annotator pairs: {doc_results['pair_count']}\")\n",
        "\n",
        "        # Display results by annotator pair\n",
        "        if doc_results['annotator_pairs']:\n",
        "            print(\"\\n  Results by annotator pair:\")\n",
        "            for pair in sorted(doc_results['annotator_pairs'], key=lambda x: x['kappa'], reverse=True):\n",
        "                print(f\"    Annotators {pair['annotator1']} and {pair['annotator2']}:\")\n",
        "                print(f\"      Overlapping spans: {pair['overlapping_spans']}\")\n",
        "                print(f\"      Kappa: {pair['kappa']:.4f}\")\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Prepare data for bar chart\n",
        "    doc_ids = [f\"Doc {doc_id}\" for doc_id in agreement_results['per_document'].keys()]\n",
        "    doc_kappas = [doc_result['avg_kappa'] for doc_result in agreement_results['per_document'].values()]\n",
        "\n",
        "    # Add overall kappa\n",
        "    doc_ids.append(\"Overall\")\n",
        "    doc_kappas.append(overall['avg_kappa'])\n",
        "\n",
        "    # Color bars based on Kappa interpretation\n",
        "    colors = []\n",
        "    for kappa in doc_kappas:\n",
        "        if kappa > 0.80:\n",
        "            colors.append('darkgreen')\n",
        "        elif kappa > 0.60:\n",
        "            colors.append('green')\n",
        "        elif kappa > 0.40:\n",
        "            colors.append('yellow')\n",
        "        elif kappa > 0.20:\n",
        "            colors.append('orange')\n",
        "        else:\n",
        "            colors.append('red')\n",
        "\n",
        "    # Create bar chart\n",
        "    bars = plt.bar(doc_ids, doc_kappas, color=colors)\n",
        "\n",
        "    # Add horizontal lines for Kappa interpretation thresholds\n",
        "    plt.axhline(y=0.20, color='r', linestyle='--', alpha=0.5, label='Poor/Fair')\n",
        "    plt.axhline(y=0.40, color='orange', linestyle='--', alpha=0.5, label='Fair/Moderate')\n",
        "    plt.axhline(y=0.60, color='y', linestyle='--', alpha=0.5, label='Moderate/Substantial')\n",
        "    plt.axhline(y=0.80, color='g', linestyle='--', alpha=0.5, label='Substantial/Almost Perfect')\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Document')\n",
        "    plt.ylabel(\"Cohen's Kappa\")\n",
        "    plt.title(f\"Average Cohen's Kappa for {attribute_name} Agreement by Document\")\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Create a second chart for category frequencies\n",
        "    if sorted_categories:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        # Prepare data (show top 10 categories)\n",
        "        top_n = min(10, len(sorted_categories))\n",
        "        top_categories = [cat for cat, _ in sorted_categories[:top_n]]\n",
        "        frequencies = [count/total_count for cat, count in sorted_categories[:top_n]]\n",
        "\n",
        "        # Create horizontal bar chart\n",
        "        bars = plt.barh(top_categories, frequencies, color='skyblue')\n",
        "\n",
        "        # Add value labels\n",
        "        for bar in bars:\n",
        "            width = bar.get_width()\n",
        "            plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "                    f'{width:.1%}', va='center')\n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Frequency')\n",
        "        plt.ylabel(attribute_name)\n",
        "        plt.title(f\"{attribute_name} Frequency Distribution\")\n",
        "        plt.xlim(0, max(frequencies) * 1.1 if frequencies else 0.1)\n",
        "        plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "08Kp9PuAClYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_category_agreement(doc_id_to_annotations):\n",
        "    \"\"\"\n",
        "    Calculate and display agreement metrics for main error categories.\n",
        "\n",
        "    Args:\n",
        "        doc_id_to_annotations: Dictionary mapping doc_ids to lists of annotations\n",
        "\n",
        "    Returns:\n",
        "        The calculated agreement metrics\n",
        "    \"\"\"\n",
        "    print(\"\\nCalculating agreement for main categories...\")\n",
        "    category_agreement = calculate_category_agreement(doc_id_to_annotations, attribute='labels')\n",
        "    display_category_agreement(category_agreement, attribute_name='Category')\n",
        "\n",
        "    return category_agreement"
      ],
      "metadata": {
        "id": "vhAThANqCoej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Kappa for Category Agreement - Visualization**"
      ],
      "metadata": {
        "id": "0w2pwicaB_qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and display Kappa for error categories\n",
        "category_agreement = analyze_category_agreement(doc_id_to_annotations)"
      ],
      "metadata": {
        "id": "l5t7l0QyCsBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}